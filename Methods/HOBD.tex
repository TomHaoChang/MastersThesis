
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage[vlined, ruled]{algorithm2e}
\usepackage{geometry}
\usepackage[noend]{algpseudocode}
\usetikzlibrary{bayesnet}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{enumitem,kantlipsum}
\usepackage{float}
\usepackage{titlesec}
\usepackage{natbib}
\usepackage{apalike}
\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\setlength{\parskip}{1em}
\geometry{letterpaper,left=1.5in,right=1in,top=1in,bottom=1in}
\setlength\parindent{0pt}
\linespread{1.5}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\tr}{tr}

\begin{document}
\pagenumbering{gobble}
{\centering
  \textbf{A Lossless and High Resolution Approach to Recovering Dynamic Brain Functional Connectivity}\par
  A Thesis\par
  Submitted to the Faculty\par
  in partial fulfillment of the requirements for the\par
  degree of\par
  Master of Science\par
  in\par
  Computer Science\par
  by\par
  Hao Chang\par
  DARTMOUTH COLLEGE\par
  Hanover, New Hampshire\par
  May, 2017\par
}
\vspace{5mm}
\setlength{\parskip}{0em}
\begin{flushright}
Examining Committee:\par
\vspace{5mm}
$\rule{5cm}{0.15mm}$\par
Hany Farid\par
\vspace{5mm}
$\rule{5cm}{0.15mm}$\par
Qiang Liu\par
\vspace{5mm}
$\rule{5cm}{0.25mm}$\par
Jeremy Manning\par
\end{flushright}
\setlength{\parskip}{1em}
\newpage


\null\par
\newpage
\pagenumbering{roman}
\section{Abstract}
Recent research focused on the analysis of brain functional connectivity---pair-wise correlation between time series of activities in different brain regions---has achieved monumental breakthroughs in improving our understanding of the human brain. Moreover, new efforts have attempted to cast functional connectivity as a dynamic process that changes from moment to moment, and have revealed that important brain dynamics information exists within these dynamic correlation structures. A tried and tested method of calculating dynamic functional connectivity is the sliding window method, which calculates the temporal correlation at a time point by finding the average correlation of neighboring time points. However, the process of averaging in the sliding window method blurs the temporal correlation structures at each moment, and thus provides only a rough approximation (or low resolution representation) of the temporal ground truth. In addition, the sliding window approach requires an extended buffer to calculate the correlation at each time point, therefore it is unable to recover the dynamic correlations at the edges of the time series, resulting in substantial data loss. In order to more effectively recover dynamic correlation structures from brain data, we present the Time Sequence Correlation Recovery (TimeCorr) method. TimeCorr calculates the correlation at each time point $t$ by multiplying the correlation components of the whole time series by a gaussian distribution centering at $t$. The use of Gaussian coefficients improves the locality of TimeCorr recovery results, and makes it possible for TimeCorr to accurately calculate the dynamic correlation of time points at the edges of the time series. To illustrate the effectiveness of TimeCorr, we compare its performance with that of the sliding window approach, and show that TimeCorr produces similar and often times more accurate and stable results across all of our test cases.

\newpage
\section{Acknowledgements}
Firstly, I would like to thank my mentors Professor Jeremy Manning of the Psychological and Brain Sciences Department at Dartmouth College and Professor Qiang Liu of the Computer Science Department at Dartmouth College. Professor Manning and Professor Liu were great inspirations for me throughout this project. Their deep understanding of their respective fields and constant flow of bright ideas really expanded my understanding of what it means to love and excel at what I want to do. Professor Manning and Professor Liu provided timely guidance whenever I was in need and constantly encouraged me to try out new ideas and reach for higher standards.

Secondly, I would like to thank Professor Hany Farid of the Computer Science Department at Dartmouth College. Professor Farid kindled my first spark of interest in Computer Science and pushed me to pursue graduate education at Dartmouth. He has been a role model for me throughout my time at Dartmouth and will always be an inspiration for me in my future pursuits in life.

Thirdly, I would like to thank Andy Heusser and Kirsten Ziman in the Contextual Dynamics Laboratory in the Neuroscience Department at Dartmouth College. Andy and Kirsten played crucial roles in helping me overcome my lack of experience in neuroscience related research. Their patient and timely assistance helped me through many difficulties at crucial stages of my research.

Fourthly, I would like to thank Dilin Wang, Jun Han and Yihao Feng in the DartML Laboratory in the Computer Science Department at Dartmouth College. Their door were always open whenever I ran into a difficult spot or had questions about my research or writing.

Finally, I must express my very profound gratitude to my parents for providing me with unfailing support and continuous encouragement throughout my years of study and through the process of researching and writing this thesis. This accomplishment would not have been possible without them. Thank you.

\newpage
\tableofcontents
\newpage
\listoffigures
\newpage
\pagenumbering{arabic}

\section{Introduction}
Human beings have an insatiable desire to understand, not only the things around us, but also ourselves. Why do we behave and respond the way we do \citep{hasson2012}? How do we learn and adapt \citep{hasson2004,hasson2005}? What do we believe or value \citep{Greene01}? How does social context affect the way we think \citep{Matthew2015}? These questions have mesmerized wise men and women for thousands of years, but the answers continue to evade us. For many, the key to understanding human behavior lies in our brain, the central hub of all of our thoughts and decision making. So, when a new technology emerged and allowed us to study the brain in a quantitative manner, it opened a door of endless possibilities for those who possesses an aptitude for quantitative analysis and a hunger to learn more about our own identity.

Functional Magnetic Resonance Imaging, often referred to as fMRI, was first discovered and applied as a brain mapping method in 1990 by Seiji Ogawa \citep{Ogawa90}. This technology then quickly popularized among Brain Science related research due to its unprecedented low health risk to subjects and its ability to accurately translate brain activities into highly-structured quantitative data \citep{Logothetis01,Friston99,Friston98}. Once abstract brain activities are converted into signs and digits, exploring patterns in the brain simply devolves into intricate mathematical problems. For example, by treating each brain fMRI image as a feature vector, Machine Learning algorithms trained on a subset of the data can learn to identify cognitive patterns (e.g., which part of a movie someone is watching), and be used to extract similar patterns from held-out brain images \citep{Norman06,peterson12,peterson17}. These applications typically treat each brain image in isolation, and attempt to decode patterns of activities associated with each of several candidate brain states. However, important stimulus-driven activities in fMRI data are often obscured by noise created by non stimulus-driven neural activities \citep{peterson11}, which greatly reduces the effectiveness of any effort to analyze relevant brain activities \citep{hasson2009}. To address this issue, new branches of research have been emerging that specifically focus on the extraction of useful information from noisy brain fMRI data by conducting a time series analysis incorporating information from the entire dataset \citep{tang2017}, and one of the most fruitful directions involves analyzing functional connectivities within the brain \citep{peterson19} \citep{peterson20}.

The brain can be divided in various ways---according to different rules of systemization (by functionality, organization, activity patterns, etc)---into a set of regions, and functional connectivity describes the correlation between the time series of activations each pair of brain regions exhibits. When we observe the brain, the neural activities can appear incredibly complex. But the mechanics of the brain are far from random, rather, the correlation patterns our brains exhibit are highly structured. Presumably, this mirrors the complex but highly structured nature of our internal thoughts and experiences. Many past studies on functional connectivity have focused on using resting state data (fMRI images taken as participants rest in the scanner with their eyes open) to estimate a single correlation matrix for each person, which provides a rich database for mapping the relations among brain processes and their contributions to perception, action and cognition \citep{peterson9,Bassett2017}. The cognitive information of each subject was then used as identifiers to compare how the relationship between brain activities and task behaviors differ or agree between subjects \citep{Turke13,Rubinov2010,peterson10}.

Recent research efforts have revealed that important cognitive information is also contained in the temporal correlational structure of time series brain activities \citep{davidson2016}, and as a result, approaches focusing on understanding the brain through analysis of dynamic brain functional connectivities begun to rapidly emerge \citep{Nigam2016,Hutchinson2013}. One of the most significant advancements comes from a recent paper by Uri Hasson's group, which casts functional connectivity as a dynamic process that changes from moment to moment depending on the specific stimulus of that moment \citep{hasson2016}. When analyzing the dynamic functional connectivities of one experimental subject in isolation, one can examine how (or whether) the correlational structure of these activity patterns (across a given set of brain regions) varies according to the cognitive task the subject is performing. The paper also presents the revolutionary Inter-Subject Functional Correlation (ISFC) analysis, which cross-examines functional connectivities across multiple subjects to isolate stimulus-dependent inter-regional correlations from noise created by random neural activities. The Contextual Dynamics Lab successfully applied dynamic functional connectivity and ISFC analyses in their most recent paper on Hierarchical Topographic Factor Analysis (HTFA), and demonstrated that functional connectivity patterns and basic brain activation patterns contain partially non-overlapping information on stimulus-driven brain dynamics \citep{jeremy2017}.

Although functional connectivity analyses of brain data has evolved significantly \citep{olaf2005,khambhati2017}, the method for calculating dynamic functional connectivities in most research is still the sliding window method, where the correlation function is applied on the brain activations of time points within a window of set length to calculate the functional connectivity at the central time point. To calculate the functional connectivities of following time points, the window is shift forward one time point at a time until the edge of the window reaches the end of the time series \citep{enrico2011,elena2012}. However, the sliding window method possesses three fundamental limitations:

Firstly, \textbf{the sliding window approach causes significant data loss.} As the sliding window approach requires an extended buffer around each time point to calculate its functional connectivity, it is unable to conduct calculations near each end of the time series. Therefore, when a sliding window setup with a window length of $L_w$ is applied on a dataset with $t$ time points, it can only retrieve the functional connectivities of $n=t-l+1$ time points, thereby losing important information on $l-1$ time points.

Secondly, \textbf{the sliding window approach only provides low resolution representation of the temporal ground truth.} As the sliding window approach directly applies the correlation function on a time window centering around the time point of interest, it is actually calculating the average functional connectivity over all time points within the time window. This averaging process provides only a low resolution representation (rough estimation) of temporal ground truth (the actual functional connectivity at each time point), which may be insufficient for moment-by-moment dynamic functional connectivity analysis.

Thirdly, \textbf{the sliding window approach lacks flexibility.} When a sliding window setup with a window length of $L_w$ is used to calculate the functional connectivities of a time series, there is loss of information for $L_w-1$ time points. Therefore, in order to retain functional connectivity information for a sufficient number of time points for further analysis, the sliding window length is restricted to a small range of values. As the performance of sliding window implementations with different window lengths varies greatly on datasets with different dynamic correlation structures, this restriction places great limitations on the recovery capabilities of the sliding window approach. In many situations, sliding window implementations with insufficient window length provide only a poor approximation of the true moment-by-moment dynamic correlation patterns.

To address these problems, we present TimeCorr, a lossless and more flexible method for functional connectivity calculation that finds its inspiration directly from the fundamental correlation function. Like the sliding window method, TimeCorr is able to recover functional connectivity from fMRI brain images with similar if not higher accuracy. But TimeCorr goes beyond the sliding window method by (a) not requiring buffers for its calculations, thereby avoiding data loss, (b) offering extra stability by using all time points in the time series for functional connectivity calculation at every time point, (c) and providing the option to balance the locality and stability of its results through adjusting its temporal resolution parameter.

To achieve the functionalities mentioned above, TimeCorr makes use of the Gaussian distribution. By attaching a Gaussian coefficient to components at each time point in the time series, TimeCorr is able to control the amount of influence each neighboring time point has on the calculation of the correlation at the Guassian center. As the Gaussian center (highest density/coefficient) is always at the time point of interest, TimeCorr guarantees to return highly accurate approximations of the temporal ground truth. In addition, the user can choose to have more stability in the results by choosing coefficients from a Gaussian distribution with a larger variance; or higher temporal resolution and more locality by choosing coefficients from a distribution with a lower variance. In the Results section, we demonstrate the effectiveness of the TimeCorr approach through comprehensive testing and evaluation on synthetic datasets with a variety of dynamic correlation structures.

In addition, we were also curious if the advantages of TimeCorr will allow us to discovery new functional connectivity patterns in real brain fMRI datasets that have been heretofore inaccessible by the sliding window method. To measure the amount of information present within the dynamic functional connectivities of a time series, we introduce decoding accuracy, a testing parameter that describes the proportion of time points in a subject's functional connectivity time series that have highest correlation with the same time point in the ISFC of all other subjects. Time points that share high similarity across all subjects have high probability of being stimulus-driven, therefore the decoding accuracy is a good evaluation of the average proportion of brain activities that are relevant to the stimulus. Furthermore, as we lack knowledge of the true functional connectivity structures of real fMRI datasets, the decoding analysis also serves as a good alternative to assess the quality of our recovery efforts. We are interested in how different TimeCorr and sliding window implementations affect the decoding accuracy (or the amount of stimulus-driven information) of the dynamic functional connectivities within real fMRI datasets. The results are shown and discussed in more detail in the Intra-Level Decoding Analysis section of our results.

Finally, past research has revealed that functional connectivity patterns and basic brain activation patterns contain partially non-overlapping information on stimulus-driven brain dynamics \citep{jeremy2017}. To verify this result with TimeCorr, we also conducted the Multi-Level Mixture Analysis. This study aims to understand how the dynamic information from raw activations and functional connectivities can be used in unison (weighted sum) to improve the decoding capabilities of existing brain fMRI datasets. In the Multi-Level Mixture Analysis section of our results, we provide a side by side comparison that highlights the improvements achieved by our mixture model. In the process of finding the mixture model that maximizes decoding accuracy, we also analyzed the optimal distribution of weights for each level of brain dynamics to explore their contribution to the optimal mixture model. The methods and findings will be discussed extensively in the Multi-Level Mixture Analysis section of our results.

\newpage
\section{Model Description}
\subsection{Single Subject TimeCorr}
The TimeCorr method was designed for the purpose of replacing the sliding window method as a lossless alternative to achieve more accurate calculations of dynamic functional connectivities (the temporal correlation at each time point between pairs of brain regions) from brain fMRI data. The sliding window method operates by applying the correlation calculation function on the activations of time points within a window of length $L_w$ and centering on the time point of interest $t$, where $L_w$ must be of considerable magnitude for the sliding window method to achieve respectable accuracy. However, due to its inherent design flaw, using the sliding window method on a dataset of time length $T$ could only yield functional connectivities for $T-L_w+1$ data points, a loss not insignificant due to the typical magnitude of $L_w$. In addition, as the sliding window method places equal weights on every time point within its window of calculation, it can only achieve a rough approximation of the average correlation within the window but not the instantaneous truth at each time point.

To solve these problems, we designed the TimeCorr method, which finds inspiration from the fundamental correlation function. Instead of isolating only a block of time points, TimeCorr effectively utilizes information from the entire dataset for functional connectivity calculation at every time point. To ensure temporal accuracy in TimeCorr's results, the components at each time point are multiplied by a weight drawn from a normalized Gaussian density function so that the influence of each time point on the calculation of functional connectivity at time point $t$ is proportional to its distance from $t$. When we observe the correlation function:
\begin{align*}
C(x,y) = \frac{\sum_{t=1}^T (x_t-\bar{x})(y_t-\bar{y})}{Ts_xs_y},
\end{align*}
the component $(x_t-\bar{x})(y_t-\bar{y})$ represents the temporal relationship between variable $x$ and variable $y$ at time point $t$. Through the application of Gaussian density coefficients, TimeCorr calculations place heavier weights on the relationship components of time points closer to the Gaussian center (the time point of interest) and lighter weights on time points further away. In comparison with the flat averaging in the sliding window approach, this adaptation provides a better representation of the dynamic correlation structure at the time point of interest (the temporal ground truth), while also taking into consideration the general dynamics throughout the entire time series. In addition, this adaptation of the correlation function effectively eliminates the need to sacrifice buffer time points at the beginning and end of the time series, thereby increasing the number of output time points to equal to the number of input time points and avoiding any data loss.

\large{\textbf{Formal Definition}}

\normalsize
Given a single subject's brain activation matrix with $T$ time points and $V$ voxels, to apply TimeCorr:

\begin{enumerate}
\item Base on the amount of influence neighboring time points should have on the functional connectivity calculation at each time point, choose an appropriate Gaussian variance $\sigma$ to generate coefficients.

\item For each time point $t$:
\begin{enumerate}
\item Using a Gaussian density function of variance $\sigma$ and mean $t$, generate an array of coefficients $w_t$ for $T$ time points, where each element $w_x$ of $w_t$ can be calculated through the following equation:
\begin{align*}
w_x = \mathcal{N}(x|t,\sigma) = \frac{1}{\sqrt{2\sigma\pi}}e^{-\frac12 \frac{(x-t)^2}{\sigma}},
\end{align*}
where $\mathcal{N}(x|t,\sigma)$ represents the Gaussian probability density of $x$ when mean is $t$ and variance is $\sigma$.

\item For each voxel $i$, element-wise multiply its activation time sequence $a_i$ by the coefficients array centered at $t$ to create a weighted activations array $S^i_t$.

\item Compute the temporal correlation between voxel $i$ and voxel $j$ at time point $t$ through the equation:

\begin{align*}
R(S^i_t,S^j_t) = \frac{1}{Z}\frac{\sum_{l=0}^T (S_l^i - \bar{S^i_t})\cdot(S^j_l - \bar{S^j_t})\cdot \mathcal{N}(l|t,\sigma)}{s_t^i \cdot s_t^j},
\end{align*}
where
\begin{align*}
\mathcal{N}(l|t,\sigma) &= \frac{1}{\sqrt{2\sigma\pi}}e^{-\frac12 \frac{(l-t)^2}{\sigma}}\\
Z &= \sum_{l=0}^T \mathcal{N}(l|t,\sigma)\\
\bar{S^i_t} &=\frac{1}{Z} \sum_{l=0}^T S^i_l \cdot \mathcal{N}(l|t,\sigma)\\
s_t^i &=\sqrt{ \frac{1}{Z}\sum_{l=0}^T (S_l^i-\bar{S_t^i})^2 \cdot \mathcal{N}(l|t,\sigma)}.\\
\end{align*}
\item Repeat the process above for every voxel pair to create a correlation matrix for time point $t$.
\end{enumerate}
\item Convert the correlation matrix at every time point to its inverse squareform format (a 1-dimensional array containing only the upper right half of the non-diagonal elements of a square matrix), outputting an array of size $T$ by $(V^2-V)/2$ dimensional matrix, where $T$ and $V$ represent the time length and voxel count of the dataset.
\end{enumerate}

The Gaussian variance parameter $V$ gives the user extra freedom to customize the desired level of resolution for temporal correlation calculation. If a large variance is chosen, the influence of neighboring time points on the functional connectivity calculation at each time point is increased, thereby adding smoothness and stability to the resulting functional connectivity time sequence and providing a more accurate representation of the general dynamics. But if a small variance is chosen, most of the weight will be placed on the time point of interest and its immediate neighbors, so the resulting functional connectivity time sequence will have more emphasis on temporal accuracy. After experimenting with different setups, we discovered that choosing a Gaussian variance equal to the minimum between 1000 and the total number of time points achieves a good balance between locality and fluidity. This finding will be discussed in more detail in the Results section.

\subsection{TimeCorr Inter-Subject Functional Connectivity}

Inter-Subject Functional Connectivity (ISFC) is the functional connectivity between brain regions of different subjects \citep{jeremy2017,hasson2016}. The patterns recovered by ISFC is analogous to single subject functional connectivities (which reflect the correlational structure across brain regions within one individual's
brain), but they should reflect only activities that are specifically stimulus-driven. For every subject, we compare its brain activations time sequence with the average activations of all other subjects. Through the process of averaging the activations of multiple subjects, we dampen random noise and emphasize activities that are common across all subjects, which are usually stimulus-dependent. Therefore, when we calculate the functional connectivity matrix between subject activation and the average response from the rest of the participants, the functional connectivities we calculate are more likely to reflect activities that are stimulus-driven.

Furthermore, after we obtain the functional connectivity matrix between each subject and the average of their counterparts, we use Fisher Z-transformation to average the results from all the subjects. In addition to the noise-dampening and stimulus-enhancing effects that comes with averaging, we are also able to gain an unbiased view of the overall stimulus-driven functional connectivity patterns across all subjects within our analysis. The Fisher Z-transformation is applied in the averaging process as a means to stabilize and reduce approximation variance and return a less biased result than that of directly averaging correlations.

\large{\textbf{Formal Definition}}

\normalsize
Given an activations dataset containing $N$ subjects, each with $T$ time points and $V$ voxels, to apply TimeCorr ISFC:
\begin{enumerate}
\item Determine the desired temporal resolution for calculation and select $\sigma$, the variance of the Gaussian density function for TimeCorr coefficients generation.
\item For each subject $s$,
\begin{enumerate}
\item Find the average activation of all other subjects:
\begin{align*}
O_s=\frac{\sum_{i\neq s}^N S_i}{N-1},
\end{align*}
where $S_i$ represents the activation matrix for subject $i$, and $N$ represents the total number of subjects.
\item Find the functional connectivity between the brain activations time series $S$ for subject $s$ and the average activations of all other subjects $O$ using TimeCorr ISFC with variance $\sigma$. The correlation between $S^i_t$---activation of voxel i for subject $s$ at time $t$---and $O^j_t$---average activation of voxel $j$ of all other subjects at time points $t$---is obtained through the following equation:
\begin{align*}
C(S^i_t,O^j_t) = \frac{1}{Z}\frac{\sum_{l=0}^T (S_l^i - \bar{S^i_t})\cdot(O^j_l - \bar{O^j_t})\cdot \mathcal{N}(l|t,\sigma)}{s_t^i \cdot o_t^j},
\end{align*}
where
\begin{align*}
\mathcal{N}(l|t,\sigma) &= \frac{1}{\sqrt{2\sigma\pi}}e^{-\frac12 \frac{(l-t)^2}{\sigma}}\\
Z &= \sum_{l=0}^T \mathcal{N}(l|t,\sigma)\\
\bar{S^i_t} &=\frac{1}{Z} \sum_{l=0}^T S^i_l \cdot \mathcal{N}(l|t,\sigma)\\
\bar{O^j_t} &=\frac{1}{Z} \sum_{l=0}^T O^j_l \cdot \mathcal{N}(l|t,\sigma)\\
s_t^i &=\sqrt{ \frac{1}{Z}\sum_{l=0}^T (S_l^i-\bar{S_t^i})^2 \cdot \mathcal{N}(l|t,\sigma)}\\
o_t^j &=\sqrt{ \frac{1}{Z}\sum_{l=0}^T (O_l^j-\bar{O_t^j})^2 \cdot \mathcal{N}(l|t,\sigma)}.\\
\end{align*}
\item Repeat the above process for every pair of voxels, for every time point to obtain a time series of correlation matrices.
\end{enumerate}
\item Element-wise apply Fisher Z-transformation $F(x)$ to the correlation matrices for each subject at each time point to obtain the corresponding Z-correlation matrices:
\begin{align*}
F(r) = \frac{1}{2}\ln\left(\frac{1+r}{1-r}\right).
\end{align*}
\item Element-wise average the Z-correlation matrices across all subjects:
\begin{align*}
S_Z = \frac{1}{N}\sum^N_{i=1}Z_i.
\end{align*}
\item Element-wise apply inverse Z-transformation $I(x)$ to the average Z-correlation matrix to obtain the TimeCorr ISFC matrix:
\begin{align*}
\text{TimeCorr ISFC} = I(S_Z) = \frac{e^{(S_Z+S_Z^T)}-1}{e^{(S_Z+S_Z^T)}+1}.
\end{align*}
\item Convert the correlation matrix at every time point to its inverse squareform format, and output a $T$ by $(V^2-V)/2$ dimensional matrix containing the average functional connectivity of all subjects in the dataset, where $T$ and $V$ represent the time length and voxel count of the dataset, respectively.
\end{enumerate}

\subsection{Multi-Subject Resampling Analysis Using TimeCor ISFC}
The Multi-Subject Resampling Analysis is a two-step method designed to evaluate the the quality of recovery of stimulus-dependent dynamic functional connectivity from an fMRI dataset by cross referencing data from multiple subjects \citep{jeremy2017}. In the first step, we conduct an Decoding Analysis on the dataset by calculating the Multi-Subject TimeCorr ISFC of two random equal divisions of the subject data, and using the results from one group as an estimation of the ground truth to evaluate the quality of recovery for each time point in the other other group. Through the process of averaging in calculation of the ISFC in each group, non stimulus-driven neural activities are dampened and activities that are common across subjects are emphasized, thereby increasing the salience of patterns that reflect activities that are only stimulus-driven. Additionally, we calculate the correlation between the dynamic functional connectivities of each time point in one group with the dynamic functional connectivities of the corresponding time point in the "ground truth". When the correlation between the same time point in both sub-groups is high, it indicates that the average brain activities in one group demonstrates very high similarity with the average brain activities of all subjects in the other group at that time point. When the brain activities of multiple subjects display high similarity for certain time points in their brain activations time sequence, then intuitively there's a high probability that their brain responses for those time points are uniformly caused by their common stimulus. Using this logic, the decoding correlation of the time series helps us visualize the quality of recovery of each time point.

Furthermore, we understand that having more smoothing on the dataset (averaging in ISFC) may also result in higher correlation between activations of the same time point across groups. In the most extreme case where the variance of TimeCorr is equal to infinity, which is equivalent to a sliding window with window length equal to the dataset time length, the recovered dynamic functional connectivity will be identical for every time point in the time series, giving us high correlation across all the time points. To compensate for this, we came up with the second part of the Resampling analysis: the Resampling Analysis.

The idea behind this analysis is that, if there is too much smoothing in the ISFC matrices (e.g., in the extreme case where every time point has the same functional connectivity), then the original time series will not always have higher correlation than a reshuffled time series. By calculating the proportion of times where a reshuffled time series gives similar or higher correlation than the original time series, we get the resampling accuracy.  Using this parameter, we can correct for the effect of smoothing in our decoding accuracy, and achieve a more realistic understanding of how well the dynamic correlation structure is being recovered.

\large{\textbf{Formal Definition}}

\normalsize
Given a multi-subject dataset containing $S$ subjects, each possessing $T$ time points and $V$ voxels, to conduct Decoding Analysis:
\begin{enumerate}
\item Decoding Analysis:
\begin{enumerate}
  \item Select a Gaussian variance $\sigma$ for TimeCorr coefficient generation.
  \item For $n$ repetitions:
  \begin{enumerate}
  \item Randomly divide the subjects into two equal groups.
  \item Calculate Inter-Subject Functional Connectivity (ISFC) for each group using TimeCorr. The resulting ISFC matrices are labelled $I_1$ and $I_2$. We arbitrarily designate $I_1$ as a rough estimation of the ground truth.
  \item Calculate correlation between the ISFC at each time point in $I_1$ and the ISFC for the corresponding time point in $I_2$, resulting in a decoding correlation matrix of length $T$.
  \item Using the decoding correlation matrix, we can visualize how well the dynamic functional connectivity at each time point has been recovered. Higher correlation at a time point represents a higher probability its dynamic functional connectivity was correctly recovered.
\end{enumerate}
\item Resampling Analysis:
\begin{enumerate}
\item Calculate the mean of the decoding correlation matrix $C$.
\item For $N$ repetitions:
\begin{enumerate}
  \item Roll the time series in $I_2$ forward so that the last time point in the time series becomes the first time point. This reshuffled time series will be labelled $I_{\text{reshuffled}}$.
  \item Calculate the mean of the correlation between each time point in $I_1$ and the corresponding time point in $I_{\text{reshuffled}}$.
\end{enumerate}
  \text{Decoding Accuracy} = \frac{\text{Total number of time points correctly decoded across all repetitions}}{\text{repetition count} * \text{time length} * 2}.
\end{align*}
\end{enumerate}
\end{enumerate}


gives an accurate representation of

\begin{enumerate}
\item The average proportion of subject activities within the dataset that are stimulus dependent.
\item The potency of subjects' brain response to the stimuli.
\item The average proportion of subject activities that shows significant distinction from the rest of the time sequence.
\item Similarity between subject responses throughout the fMRI simulation.
\item The potency of stimuli in generating salient cognitive response across all subjects.
\end{enumerate}

Due to the above characteristics, the Decoding Analysis makes up a crucial part of our project as a means to evaluate the effectiveness of our newly developed methods.




\subsection{Multi-Subject Decoding Analysis with TimeCorr ISFC}
The Multi-Subject Decoding Analysis is a method designed to find stimulus-dependent activations in an fMRI dataset by cross referencing data from multiple subjects \citep{jeremy2017}. Through the application of Multi-Subject TimeCorr ISFC on two equal divisions of the subject data, non stimulus-driven neural activities are dampened and activities that are common across subjects are emphasized, thereby increasing the salience of patterns that reflect activities that are only stimulus-driven. Additionally, when the correlation between the same time point in both sub-groups is high, it indicates that the average brain activities in one group demonstrates very high similarity with the average brain activities of all subjects in the other group at that time point. When the brain activities of multiple subjects display high similarity for certain time points in their brain activations time sequence, then intuitively there's a high probability that their brain responses for those time points are uniformly caused by their common stimulus. Using this logic, the decoding accuracy of the dataset---the percentage of time points that demonstrates highest correlation with themselves across two sub-divisions of the dataset---gives an accurate representation of

\begin{enumerate}
\item The average proportion of subject activities within the dataset that are stimulus dependent.
\item The potency of subjects' brain response to the stimuli.
\item The average proportion of subject activities that shows significant distinction from the rest of the time sequence.
\item Similarity between subject responses throughout the fMRI simulation.
\item The potency of stimuli in generating salient cognitive response across all subjects.
\end{enumerate}

Due to the above characteristics, the Decoding Analysis makes up a crucial part of our project as a means to evaluate the effectiveness of our newly developed methods.

\large{\textbf{Formal Definition}}

\normalsize
Given a multi-subject dataset containing $S$ subjects, each possessing $T$ time points and $V$ voxels, to conduct Decoding Analysis:
\begin{enumerate}
\item Select a Gaussian variance $\sigma$ for TimeCorr coefficient generation.
\item For $n$ repetitions:
\begin{enumerate}
\item Randomly divide the subjects into two equal groups.
\item Calculate Inter-Subject Functional Connectivity (ISFC) for each group using TimeCorr. The resulting ISFC matrices are labelled $I_1$ and $I_2$.
\item Calculate time-point-wise correlation between ISFC at each time point in $I_1$ and ISFC at each time point in $I_2$, resulting in a $T$ by $T$ correlation matrix.
\item A time point $t$ in $I_1$ is correctly decoded if the corresponding highest correlation time point in $I_2$ is also $t$; and vice versa for decoding time points in $I_2$.
\item Count and record the number of time points that were correctly decoded in both $I_1$ and $I_2$.
\end{enumerate}
\item Sum the total number of time points correctly decoded in $I_1$ and $I_2$ across all repetitions, and find the average through dividing the sum by the product between the repetition count and two times the time point count (to account for both $I_1$ and $I_2$). The result is the decoding accuracy of the multi-subject dataset:
\begin{align*}
\text{Decoding Accuracy} = \frac{\text{Total number of time points correctly decoded across all repetitions}}{\text{repetition count} * \text{time length} * 2}.
\end{align*}
\end{enumerate}




\newpage
\section{Results}
\subsection{Testing Single Subject TimeCorr on Synthetic Datasets}
To conduct a side-by-side comparison of the correlation recovery functionalities between the TimeCorr method and the traditional sliding window method, we used Cholesky decomposition to construct synthetic datasets with pre-defined correlation patterns. This approach takes a random matrix $X$ and an original correlation structure $R$, and uses the Cholesky transformation $C(R)$ to decompose the correlation matrix $R$ into its lower triangular components $L$ and $L'$, where
\begin{align*}
R = LL',
\end{align*}
and $L'$ represents the conjugate of $L$. By applying $L$ onto X---a matrix of uncorrelated variables---we can create a new, correlated, $Y$ matrix:
\begin{align*}
Y = C(R) \cdot X = L \cdot X
\end{align*}
Using this approach, we are able generate a time sequence of synthetic brain activations that exactly follows a pre-designated dynamically correlation structure.

Two kinds of synthetic datasets were generated to test different features of TimeCorr: (1) datasets divided into multiple time blocks, each containing different correlation structures, to test how TimeCorr performs when the correlation changes abruptly, (2) and datasets with linearly changing correlation structures to test how TimeCorr performs when the correlation changes gradually.

\subsubsection{Single Subject Synthetic Dataset with block correlations}

In datasets with block correlations, we divided the time sequence into $N$ blocks of equal time lengths. Each block possesses a different correlation that's consistent throughout the length of the block. These datasets will be used to test TimeCorr's ability handle abrupt correlation changes in a time series. To generate the block correlation dataset:
\begin{enumerate}
\item Using a random normal distribution with mean equal to 0 and variance equal to 1, generate a dataset $X$ of dimensions $T$ by $V$, where $T$ and $V$ represents the desired time length and voxel (brain region) count, respectively.
\item Design a correlation matrix of dimensions $V\times V$ for each of the $N$ blocks, denoted $R_n$.
\item For each of the $N$ blocks in $X$, transform the block $X_n$ into correlated structure using the equation:
\begin{align*}
W_n = C(R_n) \cdot X_n
\end{align*}
Where $C(R_n)$ represents the Cholesky decomposition of the correlation matrix $R_n$
\item Piecing all of the blocks together to form a new synthetic dataset W that possesses the designated dynamic correlation structure.
\end{enumerate}

To evaluate and compare the performances of TimeCorr and the sliding window method in recovering the ground truth in block correlation datasets, we used two metrics: (1) correlation with the true correlation in each block, and (2) correlation with the temporal ground truth. The first metric was chosen to demonstrate how well TimeCorr and the sliding window method can distinguish the true correlation of each block from the ground truth in other blocks. The second metric was chosen to demonstrate the overall performance of TimeCorr and the sliding window approach in recovering the temporal ground truth, especially during abrupt correlation changes at block borders.

\begin{figure}[!htb]
\includegraphics[width=1\textwidth]{../figures/SyntheticTesting/10block100t.png}
\caption{\textbf{Testing on ten 100-time-point blocks, each with different correlations.} In the first row, solid lines show the correlation between block correlations and TimeCorr results; dotted, sliding window results. The different colors represent the correlation between the true correlation in each block and the recovered results (e.g., the blue line represents the correlation between the ground truth in the first block and the recovered results). In the second row, the solid line shows correlation between temporal ground truth and TimeCorr results; dotted, sliding window results. Finally, the columns represent TimeCorr implementations with a different Gaussian variances. The boundaries of each block are indicated by semi-transparent vertical black lines.}
\label{fig:10block100t}
\end{figure}

The graphs displayed in Figure \ref{fig:10block100t} show evaluation of the average results from running TimeCorr and the sliding window method on 100 different block correlation datasets, each containing 10 blocks of 100 time points. The purpose of this dataset is to test how well TimeCorr recovers dynamic correlation structures that rapidly and abruptly changes between different states. The graphs in the first row show that the correlation between the recovered results and the ground truth of a time block peaks within the time span of the time block, but falls to zero at other time points. This result indicates that both the TimeCorr and the sliding window approach are able to easily differentiate the true correlation structure within a block from the true correlation of other blocks. In addition, we can see that results from the TimeCorr approach show different levels of locality and stability with different configurations of Gaussian variance. At low Gaussian variance, the TimeCorr results show high correlation with ground truths at time points near the center of each block where the correlation structure remains unchanged, indicating better imitations of the general structures of the true correlation matrices. In contrast, results from TimeCorr setups with high Gaussian variance demonstrates lower correlation at time points where the correlations structure is stable. We hypothesize this difference may be due to the differences in temporal resolution between TimeCorr setups of varying Gaussian variance. As TimeCorr setups with lower Gaussian variance (higher temporal resolution) place more emphasis on time points closer to the time point of interest and neglect time points further away, their temporal results are less easily influenced by later changes in correlation structures.

The graphs in the second row show the correlation between the temporal ground truth (true correlation at each time point) and the TimeCorr and sliding window results, represented by solid and dotted lines, respectively. As the Gaussian variance of TimeCorr increases, we observe big improvements in the correlation between its results and the temporal ground truths at block borders, where there is abrupt changes in the temporal correlation structure. At time points where the correlation structure is stable and unchanging (block centers), results from TimeCorr setups with low variance demonstrate high correlation with the temporal ground truths, which is consistent with the patterns in the graphs in the first row. These patterns confirm our earlier hypothesis that high resolution (low variance) TimeCorr setups demonstrates better performance when the correlation structure is stable, but TimeCorr setups with low resolution (high variance) is able to recover changing correlation patterns with better accuracy. The performance difference between high Gaussian variance and low Gaussian variance TimeCorr setups also demonstrates that utilization of information from a larger neighborhood of time points is conducive to more effective recovery of dynamically changing correlation structures, while concentrating on a smaller neighborhood of time points gives better recovery of stable correlation patterns.

Finally, the graphs show that the sliding window results are missing information for a number of time points at each end of the time series. As mentioned before, because the sliding window approach requires a sizable buffer for correlation calculation, it is unable to make calculations at the ends of the time series. This loss in information is not very obvious in this synthetic dataset as the sliding window length is very short when compared with the time length of the dataset. However, in other synthetic datasets and real datasets with shorter time lengths, the information loss because more significant and noticeable.

\begin{figure}[!htb]
\includegraphics[width=1\textwidth]{../figures/SyntheticTesting/2block50t4var4slideEB.png}
\caption{\textbf{Testing on two 50-time-point blocks, each with different correlations.} In the first row, the graph on the left shows the correlation (with error bars) between block correlations and TimeCorr results; right, sliding window results. In the second row, the graph on the left shows correlation (with error bars) between temporal ground truth and TimeCorr results; right, sliding window results. Different colors represent different TimeCorr and sliding window implementations.}
\label{fig:2block50t}
\end{figure}

To further confirm our results, we reran the tests on 100 different datasets containing 2 time blocks with different correlations, each spanning 50 time points. Evaluation of the average results from the TimeCorr and the sliding window implementations are shown in Figure \ref{fig:2block50t}, with behaviors of the ground truth displayed for comparison. From these graphs, we can confirm that TimeCorr with low Gaussian variance is able to maintain high correlation with the temporal ground truth at time points where the correlation remains unchanged. In contrast, TimeCorr with low resolution (high Gaussian variance) is able to produce slightly higher correlation and lower MSE across most time points. However, low resolution (high variance) TimeCorr predicts and reacts to future changes in temporal correlation with greater sensitivity, which results in premature changes in its results before the changes in ground truth actually take place.

When looking at the sliding window results, we notice that implementations with longer window length is generally able to produce better recovery results (higher correlation with ground truth). However, longer window length also results in greater data loss, as shown by the gradual decrease in recoverable time span in the sliding window results. In contrast, TimeCorr is able to generate similar, and sometimes even better, results without the data loss.

\begin{figure}[!htb]
\includegraphics[width=1\textwidth]{../figures/SyntheticTesting/2block11t3var3slideEB.png}
\caption{\textbf{Testing on two 11-time-point blocks, each with different correlations.} In the first row, the graph on the left shows the correlation (with error bars) between block correlations and TimeCorr results; right, sliding window results. In the second row, the graph on the left shows correlation (with error bars) between temporal ground truth and TimeCorr results; right, sliding window results. Different colors represent different TimeCorr and sliding window implementations.}
\label{fig:2block11t}
\end{figure}

To explore how TimeCorr performance varies with respect to the relationship between TimeCorr variance and the time span of each block, we conducted another analysis on 100 different datasets containing 2 time blocks with different correlations, each spanning only 11 time points. Evaluation of the average results from the TimeCorr and the sliding window implementations are shown in Figure \ref{fig:2block11t}. The graphs in this figure show that when the TimeCorr variance is greater than the time length of each block, the recovery capabilities of TimeCorr starts to deteriorate. Intuitively, the best way to calculate the correlation within a time block is to only use the time points within the block. Therefore, when the variance is too great, TimeCorr calculations place more weights on time points outside the time block, resulting in deviation from the correct temporal correlation structure. Furthermore, we notice that the TimeCorr setup that generated the best results is the one with variance roughly equal to the length of the time block. This result indirectly tells us that, if a dataset displays similar dynamic correlation structures as the block correlation synthetic datasets, we can extrapolate the lengths of the time blocks in the time series by conducting a cross-validation analysis across TimeCorr implementations with different variances.

In conclusion, as low Gaussian variance (high temporal resolution) TimeCorr implementations can achieve accurate and stable recovery of correlation structures that remain constant over extended blocks of time points and only suffer marginal accuracy loss at abrupt correlation shifts, it is more suitable for fMRI datasets where the dynamic correlation structures of brain activities are similar to that of the block correlation synthetic datasets. In addition, when compared with the sliding window approach, high resolution TimeCorr implementations also boast slightly better recovery performance in general with the additional advantage of no information loss.

\subsubsection{Single Subject Synthetic Dataset with ramping correlations}

In these datasets, we adopt a correlation structure that linearly changes from one terminal correlation $C_1$ to another terminal correlation $C_2$. These datasets will be used to test TimeCorr's ability to accurately recover correlation structures that are dynamically changing over time. To generate a ramping correlation dataset:
\begin{enumerate}
\item Using random normal distribution, generate a dataset X of dimensions $T$ by $V$, where $T$ and $V$ represent the desired time length and voxel (brain region) count, respectively.
\item Generate two terminal correlation matrices $C_1$ and $C_2$, then generate temporal correlation structures at time point $t$ using the function
\begin{align*}
C_t = I\left(\frac{(T-t) \cdot F(C_1)}{T} + \frac{t\cdot F(C_2)}{T}\right)
\end{align*}
where T is the total time length of the datase. $F(x)$ and $I(x)$ represent Fisher Z-transformation and inverse Fisher Z-transformation, respectively. These transformations were applied to ensure stable combination of correlation structures.
\item For activations $X_t$ at each time point in X, transform the data into correlated structure using the Cholesky transformation $C(x)$:
\begin{align*}
W_t = C(C_t) \cdot X_t
\end{align*}
\item Piece everything together to form the new synthetic dataset W that possesses the designated dynamic correlation structure.
\end{enumerate}

To evaluate and compare the performances of TimeCorr and the sliding window method in recovering the ground truth in ramping correlation datasets, we used four metrics: (1) correlation with the two terminal ground truths, and (2) correlation with the temporal ground truth. The first metric was chosen to demonstrate how well TimeCorr and the sliding window method can recover the pattern of linear transition between the two terminal ground truths. The second metric was chosen to demonstrate the overall performance of TimeCorr and the sliding window approach in recovering the temporal ground truth.

\begin{figure}[!htb]
\includegraphics[width=1\textwidth]{../figures/SyntheticTesting/ramp1000t6var6slideEB.png}
\caption{\textbf{Testing different TimeCorr setups on 1000-timepoint ramping datasets.} In the first row, the graph on the left shows the correlation (with error bars) between ground truths at end points and TimeCorr results; right, sliding window results. In the second row, the graph on the left shows correlation (with error bars) between temporal ground truth and TimeCorr results; right, sliding window results. Different colors represent different TimeCorr and sliding window implementations.}
\label{fig:ramp1000t4var}
\end{figure}

First, 100 ramping datasets with 1000 time points were generated to compare how TimeCorr and the sliding window method perform on dynamic correlation structures that gradually linear transitions between two distinct correlation structures. Evaluation of the average testing results are displayed in Figure \ref{fig:ramp1000t4var}. Both methods were able to accurately recover the linear transition patterns between the two terminal ground truth (correlation improves with respect to one terminal correlation but decreases with respect to the other terminal correlation, forming an obvious cross shape). However, as the Gaussian variance of TimeCorr increased, we observe overall improvements in the correlation between the TimeCorr results and the temporal ground truths. This phenomenon is consistent with the patterns we observed in our block correlation synthetic dataset test results, where TimeCorr setups with high Gaussian variance is able to recover dynamically changing correlation structures with much higher accuracy than low variance TimeCorr implementations. However, when we increased TimeCorr's Gaussian variance beyond the time length of the dataset (1000 time points), we saw marginal improvements in TimeCorr's recovery performances. We hypothesize that because all the time points in the time series are already effectively utilized by TimeCorr when the Gaussian variance is equal to the time length of the dataset, increasing the Guassian variance further will have little to no effect on TimeCorr's result.


\begin{figure}[!htb]
\includegraphics[width=1\textwidth]{../figures/SyntheticTesting/ramp500t5var5slideEB.png}
\caption{\textbf{Testing different TimeCorr setups on 500-time-point ramping datasets.} In the first row, the graph on the left shows the correlation (with error bars) between ground truths at end points and TimeCorr results; right, sliding window results. In the second row, the graph on the left shows correlation (with error bars) between temporal ground truth and TimeCorr results; right, sliding window results. Different colors represent different TimeCorr and sliding window implementations.}
\label{fig:ramp500t4var}
\end{figure}

The performance advantage of low resolution (high variance) TimeCorr setups on recovering dynamically changing correlation structures is further verified by the results from testing on 100 ramping datasets of 500 time points. As the temporal correlation linearly transitions between the two terminal correlations within only 500 time points, the rate of change at each time point is much greater than that of the 1000 time point dataset. Evaluation of the average testing results are displayed in Figure \ref{fig:ramp500t4var}. In contrast to the results from the 1000 time point dataset, the performance difference between different resolution TimeCorr implementations is more evident in the results from the 500 time point datasets. When the Gaussian variance of TimeCorr is increased from 10 time points to 500 time points and beyond, we observe greater improvements in TimeCorr's performance than in the results from the 1000 time point ramping dataset. This pattern confirms the advantage of incorporating information from a wider neighborhood of time points in the recovery of dynamically changing correlations structures. However, when we increased TimeCorr's Gaussian variance beyond the time length of the dataset (500 time points), we saw marginal improvements in TimeCorr's recovery performances. This result confirms our earlier hypothesis that TimeCorr setups with variance equal to dataset time length gives the most optimal performance on datasets with ramping correlation structures.


\begin{figure}[!htb]
\includegraphics[width=1\textwidth]{../figures/SyntheticTesting/ramp100t4var4slideEB.png}
\caption{\textbf{Testing different TimeCorr setups on 100-time-point ramping datasets.} In the first row, the graph on the left shows the correlation (with error bars) between ground truths at end points and TimeCorr results; right, sliding window results. In the second row, the graph on the left shows correlation (with error bars) between temporal ground truth and TimeCorr results; right, sliding window results. Different colors represent different TimeCorr and sliding window implementations.}
\label{fig:ramp100t4var}
\end{figure}

To further explore the characteristics of different resolution TimeCorr setups in recovering highly dynamic correlation structures, we conducted analysis on 100 ramping datasets of only 100 time points, where the linear transitions between the two distinct terminal correlation structures are further accelerated from the 500 time point datasets. Evaluation of the average testing results are displayed in Figure \ref{fig:ramp100t4var}, and confirm our hypothesis from the previous tests: the performance of TimeCorr bottlenecks when its Gaussian variance approaches the time length of the dataset. As the Gaussian of variance of TimeCorr exceeds the total time length of 100 time points, we start to see a deterioration in the cross pattern in the first row.

Finally, summarizing the patterns observed from the test results across the 1000 timepoint, 500 time point and 100 time point datasets, we conclude that high Gaussian variance (low resolution) TimeCorr setups are advantageous in recovering continuously changing correlation structures. In addition, low resolution TimeCorr implementations show optimal performance when its Gaussian variance is equal to the total time length of the dataset. Thus, if the correlation dynamics of an fMRI dataset shows continuous change instead stable blocks, it is most advantageous to use a TimeCorr setup with a Gaussian variance equal the lesser between the total time length and 1000 time points for recovery of its dynamic correlation structure.

Testing results from all three ramping synthetic datasets also demonstrate that the performance of sliding window generally increases as the window length increases, but the rate of improvement decreases dramatically at implementations with longer window length (marginal improvement when the window length approaches $50\%$ of the dataset time length). In addition, as the window length increases, the correlation between the sliding window result and temporal ground truths gradually nears the performance of the optimal TimeCorr implementations. However, longer sliding window length also results in more significant data loss. In some cases, in order for the sliding window method to reach the same level of performance as TimeCorr, information on more than $50\%$ of the time points in the time series are lost. This tradeoff is highly impractical. Therefore, as TimeCorr is lossless, more flexibly and generally gives better performance, it is more advantageous to use TimeCorr to calculate dynamic functional connectivities of the brain.

\subsection{Testing Inter-Subject Functional Connectivity using TimeCorr}
Confident with TimeCorr's ability to recover the dynamic correlation structures of single subject datasets, we proceeded to test TimeCorr's performance in recovering inter-subject functional connectivity (ISFC) in multi-subject synthetic datasets. In practical scenarios, fMRI datasets always contain a significant amount of noise from non stimulus-driven neural activities. Therefore, to simulate realistic human brain activities, we added different levels of random noise to the synthetic datasets to gauge the robustness of TimeCorr ISFC in recovering stimulus-related functional connectivity. The noise levels we experimented with were on the order of magnitudes of $10\%$ (0.1), $100\%$(1) and $1000\%$(10) of the average activation magnitude. Evaluation of results from running TimeCorr with Gaussian variance of 300 time points and a sliding window setup with a window length of 101 time points on 100 ramping datasets, each containing 300 time points, are demonstrated in Figure \ref{fig:t300slide25var1000}.

\begin{figure}[!htb]
\includegraphics[width=1\textwidth]{../figures/ISFC/ramp300t300var101slideEB.png}
\caption{\textbf{Testing ISFC on 300-time-point ramping datasets with different noise levels.} In the first row, the graph on the left shows the correlation (with error bars) between ground truths at end points and TimeCorr results; right, sliding window results. In the second row, the graph on the left shows correlation (with error bars) between temporal ground truth and TimeCorr results; right, sliding window results. Different colors represent different level of noises.}
\label{fig:t300slide25var1000}
\end{figure}

The graphs show that TimeCorr is able to retrieve the stimulus-driven dynamic correlation structure with relatively high accuracy at low (0.1) to medium (1) noise levels, but fails to recover any meaningful information when the noise level is too high (10). These results highlight the difficulty of recovering stimulus-driven correlation structures when the subjects display weak cognitive response to the stimulus. However, when the stimuli is able to evoke reasonably strong cognitive response from the subject, TimeCorr ISFC can effectively recover the stimuli-driven dynamic correlation structures with relatively high accuracy.

In addition, due to the limited number of total time points in the dataset, we chose to use a sliding window setup with a window length of 101 time points. When comparing the correlation and MSE between their results and the temporal ground truths, the TimeCorr approach distinctly outperforms the sliding window method in every situation, even when the noise level is high. Furthermore, as the amount of noise increases, the performance of the sliding window approach deteriorates more noticeably than that of the TimeCorr approach. These contrasts further consolidate the advantageous ability of TimeCorr to produce more stable and accurate recovery of dynamic correlation structures by incorporating a wider range of time points, making it the superior choice for functional connectivity calculations.

\begin{figure}
\includegraphics[width=1\textwidth]{../figures/ISFC/2block150tslide101var150EB.png}
\caption{\textbf{Testing ISFC on 300-time-point block datasets with different noise levels.} Each time block contains 150 time points. In the first row, the graph on the left shows the correlation (with error bars) between ground truths at end points and TimeCorr results; right, sliding window results. In the second row, the graph on the left shows correlation (with error bars) between temporal ground truth and TimeCorr results; right, sliding window results. Different colors represent different level of noises.}
\label{fig:t300slide25var159}
\end{figure}

\clearpage
\newpage
\subsection{HOBD Intra-Level Decoding Analysis}
The analyses on synthetic datasets demonstrated the advantages of using TimeCorr for the recovery of single-subject functional connectivities and ISFC from noisy synthetic datasets. These advantages enable the HOBD model to explore brain dynamics through the lens of high-order functional connectivities. For the next step, we conducted the HOBD Intra-Level Decoding Analysis (ILDA), which evaluates how much stimulus-driven brain dynamics information is contained within each level of high-order ISFC obtained using the HOBD recovery methods (TimeCorr and TimeCorr Level Up). Unlike previous tests with synthetic datasets, the ground truths for each order of functional connectivity in actual fMRI datasets are unknown. Therefore, we used the decoding accuracy as a noisy proxy to roughly estimate the quality of functional connectivity recoveries and the quantity of stimulus-driven information within each order of functional connectivity. Firstly, for each subject, we used the TimeCorr Level Up function on their PCA reduced activations to calculate 9 levels of high-order functional connectivity as new activations of the corresponding order (e.g., for each subject, $9^{th}$ order functional connectivity is redefined as $9^{th}$ order activations). Next, we conducted decoding analysis on the ISFC of the subject activations at each order, from PCA reduced raw activations ($0^{th}$ order ISFC) and its ISFC ($1^{st}$ order ISFC) to the ISFC of the 9th order functional connectivities/activations ($10^{th}$ order ISFC). Three TimeCorr setups were implemented---each with Gaussian variances of $\text{min}(1000,\text{time length})$ time points, 10 time points, or 1 time point---to comprehensively understand the decoding capability of each level under varying resolutions.

Three different datasets were used for this analysis: Sherlock, Pieman, and Forrest. Detailed descriptions for each dataset are included in each subsection. For each dataset, we conducted 100 repetitions of decoding analysis with different random group divisions for each TimeCorr setup. All of the following results are the average decoding accuracy of 100 repetitions.

\subsubsection{Description for the Pieman Dataset}
This dataset was collected by Uri Hasson's lab in 2016 \citep{hasson2016}. The stimuli for this experiment were derived from a 7 minute recording of the ``Pie Man" story by Jim O'Grady. The experimental subjects were all native English speakers with normal hearing. Four testing conditions with random subject assignments were implemented: 36 subjects listened to the story from beginning to end, and were labelled as the ``intact" group; 18 subjects listened to a version of the story where the paragraphs were scrambled randomly, and were labelled as the ``paragraph" group; 36 subjects listened to a version of the story where the words were scrambled randomly, and were labelled as the ``word" group; lastly, 36 subjects did not listen to anything and stayed in resting state throughout the experiment, and were labelled as the ``resting" group. 12 seconds of neutral music and 3 seconds of silence preceded and 15 seconds of silence followed each playback in all conditions. The brain activities of all subjects for the duration of the experiment were recorded into fMRI datasets, with the data from music and silence periods discarded from all analyses.

\subsubsection{Description for the Forrest Dataset}
This dataset was collected by Michael Hanke and coleagues in 2014\citep{Hanke2014}. The stimuli for this experiment is a two-hour presentation of an audio-only version of the Hollywood feature film ``Forrest Gump" made specifically for a visually impaired audience. The movie was divided into eight 15-minute sessions with breaks in-between, and presented to 20 subjects with normal hearing. During each session, brain activities centered on the auditory cortices in both brain hemisphere and the frontal and posterior portions of the brain were recorded into high resolution 7-Tesla fMRI datasets.

\subsubsection{Description for the Sherlock Dataset}
This dataset was collected by Uri Hasson's Lab in 2017 \citep{Chen2017}. Sixteen participants (17 were described in the original paper, but one was removed from this dataset due to a small amount of missing data at the end of the movie scan) were presented with a 50-minute segment of the audio-visual movie Sherlock (BBC) while undergoing fMRI scan. Following the movie, participants were asked to describe, without any external guidance, what they recalled of its content in as much detail as they could while undergoing brain imaging. Participants were allowed to speak on any aspect of the movie for as long as they wished, while their speech was recorded with an fMRI-compatible microphone.

\subsubsection{Results}
\begin{enumerate}
\item The average results for each TimeCorr setup implemented on the Pieman dataset are displayed in Figures \ref{fig:piemanDC1000}, \ref{fig:piemanDC10}, and \ref{fig:piemanDC1}.
\item The average results for each TimeCorr setup implemented on the Forrest dataset are displayed in Figure \ref{fig:forrestDC1000}, \ref{fig:forrestDC10}, and \ref{fig:forrestDC1}.
\item The average results for each TimeCorr setup implemented on the Sherlock dataset are displayed in Figure \ref{fig:sherlockDC1000}, \ref{fig:sherlockDC10}, and \ref{fig:sherlockDC1}.
\end{enumerate}

\begin{figure}[!htb]
\caption{ILDA on TimeCorr with variance equal to total time length (Pieman)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/1000V/pieman.png}
\label{fig:piemanDC1000}
\caption{ILDA on TimeCorr with variance of 10 time points (Pieman)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/10V/pieman.png}
\label{fig:piemanDC10}
\caption{ILDA on TimeCorr with variance of 1 time point (Pieman)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/1V/pieman.png}
\label{fig:piemanDC1}
\end{figure}

Firstly, when comparing our results in Figures \ref{fig:piemanDC1000}, \ref{fig:piemanDC10}, and \ref{fig:piemanDC1} with that previous efforts, we can see that the decoding accuracy we achieved on the first order ISFC for the Pieman dataset using TimeCorr-based methods is much higher across all the TimeCorr setups \citep{jeremy2017}. As higher decoding accuracy is a strong indication of the richness of stimulus-driven activities within a dataset, our improved results confirm that TimeCorr-based methods are superior in the recovery of stimulus-driven correlation structure from fMRI datasets. Furthermore, the decoding accuracy we achieved on PCA reduced fMRI data is significantly higher than the decoding accuracy obtained from HTFA reduced datasets, indicating that PCA is more effective in preserving stimulus-driven brain dynamics than HTFA.

Secondly, we noticed that the decoding accuracy of the first order ISFC exceeds the decoding accuracy of PCA reduced brain activations in some situations, suggesting that there might be more stimulus-related brain dynamics information contained within functional connectivities than in raw activations. However, we also noticed that the decoding accuracies of higher-order ISFCs fall off almost exponentially after the first order ISFC, a behavior that's also observed in the results from the Sherlock and the Forrest datasets. Although there is a possibility that less brain dynamics information is contained at these levels, we speculate that the main factor causing the decrease in decoding accuracy is information loss, which can be caused by PCA dimensionality reduction and/or TimeCorr Gaussian averaging.

To test the effect of TimeCorr Gaussian averaging on information loss at each "level up", we implemented three TimeCorr setups with different Gaussian variances. The results from the Pieman dataset show that the decoding accuracy of lower orders of ISFC increases drastically when the TimeCorr Gaussian variance is decreased from the dataset time length to 10 time points, but undergoes a decrease when the Gaussian variance is decreased further to 1 time point. Additionally, we observe that, as the TimeCorr Gaussian variance decreases from the dataset time length to 1 time point, the decoding accuracy of higher order ISFCs of the Pieman dataset seem to show a marginal increase, indicating that Gaussian averaging inside TimeCorr may be one of the causes for information loss at high levels. However, the increase in decoding accuracy for low variance TimeCorr implementations is not replicated by the results from the Forrest nor the Sherlock dataset, which suggests that Gaussian averaging may not be the biggest factor causing the information loss.

Lastly, when comparing between results under different stimulus conditions (intact, paragraph, word, resting) within the Pieman dataset, we can see that as the complexity and integrity of the stimulus decreases, the decoding accuracy decreases for all orders of functional connectivity. This makes sense as the human brain should intuitively produce more potent cognitive response to more coherent and meaningful stimuli (intact), weaker response to scrambled and irrational stimuli (paragraph, word), and little to no response to no stimuli (resting). One anomaly exists in the results from the TimeCorr implementation with a Gaussian variance of 1 time point, where the decoding accuracy of the first order ISFC in the paragraph condition exceeded that of the intact condition. However, this behavior becomes understandable after taking into consideration the results from the TimeCorr implementation with a Gaussian variance of 10 time points: while the decoding accuracy for the first order ISFC of the paragraph condition only showed marginal decrease, the decoding accuracy of the first order ISFC of the intact condition decreased drastically when the TimeCorr Gaussian variance changed from 10 time points to 1 time point. When the temporal resolution of TimeCorr is too high (Gaussian variance of 1 time point), TimeCorr will tunnel-vision on a very narrow neighborhood of time points for its functional connectivity calculations, and thus fail to incorporate enough information to recover any meaningful dynamic correlation patterns. As the functional connectivity under the intact condition possesses more structure and coherency, it may be suffer greater deterioration than the functional connectivity from the paragraph condition when subjected to over-high temporal resolution, thus explaining the anomaly.

\begin{figure}[!htb]
\caption{ILDA on TimeCorr with variance equal to total time length (Forrest)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/1000V/forrest.png}
\label{fig:forrestDC1000}
\caption{ILDA on TimeCorr with variance of 10 time points (Forrest)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/10V/forrest.png}
\label{fig:forrestDC10}
\caption{ILDA on TimeCorr with variance of 1 time point (Forrest)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/1V/forrest.png}
\label{fig:forrestDC1}
\end{figure}

The results from the Forrest dataset, shown in Figures \ref{fig:forrestDC1000}, \ref{fig:forrestDC10}, and \ref{fig:forrestDC1}, show very similar patterns as the results from the Pieman dataset. We observe that the decoding accuracy of first order ISFC increases greatly when the Gaussian variance is decreased from the dataset time length to 10, but decreases when the Gaussian variance is decreased further to 1. Also, when the Gaussian variance is equal to 10 time points and 1 time point, we observe that the decoding accuracy of the first order ISFC exceeds that of the PCA reduced brain activations, which confirms our previous hypothesis that high order brain connectivities may contain an unexpected amount of information on brain dynamics. Finally, the decrease in decoding accuracy for high order ISFCs is consistent with the results from the Pieman dataset, suggesting that there is a high probability of information loss at each ``level up". However, unlike the results from the Pieman dataset, we do not see an increase in decoding accuracy when the TimeCorr variance is decreased from the total time length to 1 time point, which verifies our previous speculation that Gaussian averaging in TimeCorr is not the main cause of information loss.

\begin{figure}[!htb]
\caption{ILDA on TimeCorr with variance equal to total time length (Sherlock)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/1000V/sherlock.png}
\label{fig:sherlockDC1000}
\caption{ILDA on TimeCorr with variance of 10 time points (Sherlock)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/10V/sherlock.png}
\label{fig:sherlockDC10}
\caption{ILDA on TimeCorr with variance of 1 time point (Sherlock)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/1V/sherlock.png}
\label{fig:sherlockDC1}
\end{figure}

The results from the Sherlock dataset, shown in Figures \ref{fig:sherlockDC1000}, \ref{fig:sherlockDC10}, and \ref{fig:sherlockDC1}, demonstrate slightly different patterns from the results from the Forrest dataset. We observe that the decoding accuracy of first order ISFC not only increases greatly when the Gaussian variance is decreased from the dataset time length to 10 time points, but also increases further when the Gaussian variance is decreased to 1 time point. However, like the results from the Pieman and the the Forrest datasets, we observe that the decoding accuracy of the first order ISFC exceeds that of the PCA reduced brain activations when the Gaussian variance is equal to 10 time points and 1 time point. We also observe an exponential decrease in decoding accuracy for high-order functional connectivities, which barely changes when the Gaussian variance of TimeCorr is decreased from dataset time length to 1 time point.

To summarize, one important phenomenon that is observed in the results from all three datasets is the decoding accuracies from the TimeCorr implementations with a Gaussian variance of 10 time points are generally higher that that of the TimeCorr implementations with Gaussian variance equal to the dataset time length. As higher decoding accuracy is indicative of functional connectivities of a greater proportion of time points being correctly recovered, the conclusions we drew from synthetic dataset testing tell that us that the dynamic correlation patterns in the Pieman, Forrest and Sherlock datasets may more closely resemble that of the block correlations synthetic datasets. However, as discussed previously, when the TimeCorr's temporal resolution is too high (1 time point), it may fail to include enough global information to recovery meaningful dynmaic correlation structures (Pieman and Forrest).

Another important observation that is consistent across results from all three datasets is that the higher order ISFCs actually produced lower decoding accuracy. This result was unexpected as brain dynamics across subjects should intuitively become more similar at higher orders. One possible explanation for this is, although the high-order ISFC still contain information about stimulus-driven brain activities, there seems to have been a small amount of information loss in each level up process, leading to slightly lower classification accuracy. However, although higher order ISFCs display weaker decoding capabilities than ISFCs at lower level, our hypothesis is that each order of ISFC contains distinct information about brain dynamics. If this is the case, then a mixture of all orders of ISFC will potentially have stronger decoding performance than that of ISFC at any single level.

We also suspected that TimeCorr temporal resolution had some influence on the results, which is why our analysis of each dataset consists of a variety of different TimeCorr setups. We hypothesize that if the Gaussian averaging process in TimeCorr caused information loss at each ``level up", the loss would intuitively grow exponentially as we reach higher orders of functional connectivity. Therefore, TimeCorr with Gaussian variances of dataset time length, 10 time points and 1 time point were chosen to explore if variation in TimeCorr temporal resolution would affect the distribution of decoding accuracy across different levels. However, looking at the results across all three datasets, we do not notice a significant decrease in the rate of decoding accuracy deterioration when the TimeCorr resolution shifts from a variance of dataset time length to a variance of 1 time point. On the contrary, there does not seem to be a linear relationship between the decoding accuracy at higher levels and the temporal resolution of TimeCorr: e.g., the decoding accuracy of first order ISFC in the Pieman dataset increased when TimeCorr variance decreased from dataset time length to 10 time points, but decreased when the TimeCorr variance decreased further to 1 time point (a phenomenon also observed in the Forrest results). These observations indicate that the Guassian averaging process in TimeCorr is not the main culprit of information loss.

\subsection{HOBD Multi-Level Mixture Analysis}
Although the HOBD ILDA revealed that information of high-order brain dynamics may not have been properly represented by the HOBD model due to information loss, it presented the possibility that the functional connectivities at each level may represent non-overlapping sources of stimulus-relevant cognitive information that could potentially complement each other. Therefore, we designed the Multi-Level Mixture Analysis (MLMA). This analysis constructs a mixture model incorporating information from multiple orders of ISFC, and compares its decoding capabilities with that of ISFC at each individual level.

Given a fMRI dataset for $S$ subjects, each possessing $T$ time points and $V$ voxels, to construct the Multi-Level Mixture Model:
\begin{enumerate}
\item Choose the desired number of levels of functional connectivities $L$ and the appropriate Gaussian variance for TimeCorr.
\item Use the TimeCorr Level Up method to obtain all level functional connectivites for each subject.
\item Randomly divide the subjects in to four equal groups $A_1$, $A_2$, $B_1$ and $B_2$.
\item For each group, use the Multi-Subject TimeCorr ISFC method to compute the inter-subject functional connectivity for each level
\item Compute correlation matrices for each level for A and B (i.e. for group A, correlate $A_1$'s features at each time point with $A_2$'s features at each time point; similarly for group B). This yields one correlation matrix for group A and another for group B, for each level (including level 0). So if $L=10$, this process would yield 11 correlation matrices for A and another 11 for B.
\item Apply Fisher Z-transform $F(x)$ on all of the correlation matrices. The transformed correlation matrices are labelled $z_{A}^0, z_{A}^1, z_{A}^2, ..., z_{B}^0, z_{B}^1, z_{B}^2, ..., z_{B}^{10}$, where $z_{A}^0$ represent the Fisher Z-transformed correlation matrix for level 0 of group A.
\item Use constrained optimization by linear approximation (COBYLA) on the correlation matrices of group A to compute the optimal weight matrix w to be used for decoding, where $w$ will be applied to the Fisher Z-transformed matrices in the manner of the following equation:
\begin{align*}
\text{decoding\_matrix\_A} = I(w[0]*z_A^0 + w[1]*z_A^1 + w[2]*z_A^2 + ... + w[10]*z_A^{10})
\end{align*}
where $I(x)$ is the Inverse Fisher Z-transformation to convert the weighted sum into the its proper correlation form. Intuitively, this step seeks to find the w that maximizes the decoding accuracy using decoding\_matrix\_A.
\item Use the same I(weighted z-transformed sum) formula and the optimal weights from the previous step to compute decoding accuracy for group B
\item Output w and group B's decoding accuracy
\end{enumerate}

Three TimeCorr setups of varying temporal resolutions were implemented---each with Gaussian variances equal to $\text{min}(\text{dataset time length},1000)$ time points, 10 time points, or 1 time point---to comprehensively understand the decoding capability of the mixture model and each individual level under varying resolutions. In addition, similar to the single level decoding analysis conducted previously, three different datasets were used for this analysis: Sherlock, Pieman, and Forrest. For each TimeCorr setup for each dataset, we conducted 100 repetitions of decoding analysis with different random group divisions. All of the following results are the average of 100 repetitions.

Results:
\begin{enumerate}
\item The average results for each TimeCorr setup implemented on the Pieman dataset are displayed in Figure \ref{fig:piemanMM1000}, \ref{fig:piemanMM10}, and \ref{fig:piemanMM1}.
\item The average results for each TimeCorr setup implemented on the Forrest dataset are displayed in Figure \ref{fig:forrestMM1000}, Figure \ref{fig:forrestMM10} and Figure \ref{fig:forrestMM1}.
\item The average results for each TimeCorr setup implemented on the Sherlock dataset are displayed in Figure \ref{fig:sherlockMM1000}, Figure \ref{fig:sherlockMM10} and Figure \ref{fig:sherlockMM1}.
\end{enumerate}

\begin{figure}[!htb]
\caption{MLMA on TimeCorr with variance equal to total time length (Pieman)}
\centering
\includegraphics[width=1\textwidth]{../figures/Multi-Level-Mixture-Analysis/1000V/pieman-weights.png}
\includegraphics[width=1\textwidth]{../figures/Multi-Level-Mixture-Analysis/1000V/pieman-accuracy.png}
\label{fig:piemanMM1000}
\end{figure}

\begin{figure}[!htb]
\caption{MLMA on TimeCorr with variance of 10 time points (Pieman)}
\centering
\includegraphics[width=1\textwidth]{../figures/Multi-Level-Mixture-Analysis/10V/pieman-weights.png}
\includegraphics[width=1\textwidth]{../figures/Multi-Level-Mixture-Analysis/10V/pieman-accuracy.png}
\label{fig:piemanMM10}
\end{figure}

\begin{figure}[!htb]
\caption{MLMA on TimeCorr with variance of 1 time point (Pieman)}
\centering
\includegraphics[width=1\textwidth]{../figures/Multi-Level-Mixture-Analysis/1V/pieman-weights.png}
\includegraphics[width=1\textwidth]{../figures/Multi-Level-Mixture-Analysis/1V/pieman-accuracy.png}
\label{fig:piemanMM1}
\end{figure}

\begin{figure}[!htb]
\caption{MLMA on TimeCorr with variance equal to total time length (Forrest)}
\centering
\includegraphics[width=0.8\textwidth]{../figures/Multi-Level-Mixture-Analysis/1000V/forrest.png}
\label{fig:forrestMM1000}
\caption{MLMA on TimeCorr with variance of 10 time points (Forrest)}
\centering
\includegraphics[width=.8\textwidth]{../figures/Multi-Level-Mixture-Analysis/10V/forrest.png}
\label{fig:forrestMM10}
\caption{MLMA on TimeCorr with variance of 1 time point (Forrest)}
\centering
\includegraphics[width=.8\textwidth]{../figures/Multi-Level-Mixture-Analysis/1V/forrest.png}
\label{fig:forrestMM1}
\end{figure}


\begin{figure}[!htb]
\caption{MLMA on TimeCorr with variance equal to total time length (Sherlock)}
\centering
\includegraphics[width=.8\textwidth]{../figures/Multi-Level-Mixture-Analysis/1000V/sherlock.png}
\label{fig:sherlockMM1000}
\caption{MLMA on TimeCorr with variance of 10 time points (Sherlock)}
\centering
\includegraphics[width=.8\textwidth]{../figures/Multi-Level-Mixture-Analysis/10V/sherlock.png}
\label{fig:sherlockMM10}
\caption{MLMA on TimeCorr with variance of 1 time point (Sherlock)}
\centering
\includegraphics[width=.8\textwidth]{../figures/Multi-Level-Mixture-Analysis/1V/sherlock.png}
\label{fig:sherlockMM1}
\end{figure}

With each implementation of the Multi-Level Mixture Model, the subjects are divided into two equal groups, one for training and one for testing. As a result, we observe that training and testing using only half of the subjects caused the decoding accuracy to be a lot lower than when using the entire dataset. From a mathematical perspective, averaging over more subjects brings more stability to the dataset by smoothing out random noise in the dataset and adding emphasis to activities common to all subjects, which are typically stimulus-driven. Therefore, when less subjects are used in the calculation of inter-subject functional connectivity (ISFC), the de-noising effect of averaging is weakened and less emphasis is put on activities that are stimulus-driven, thus causing lower decoding accuracy.

One of the most important observations from the results of Multi-Level Mixture Analysis across all three datasets is that the Mixture Model produces much higher decoding accuracy than any single level of ISFC. When taking into consideration our hypothesis from the decoding analysis in the previous section, this is a very direct and compelling proof that different orders of ISFC, although not recovered with perfect fidelity, still contains important non-overlapping information about brain dynamics and, although individual levels did not produce high classification accuracy, when used in unison can greatly enrich data complexity and increase decoding capabilities.

The importance of each level is further confirmed by optimal weight distribution across all three datasets. While the first order ISFC always has the highest weight, the weight distribution between PCA reduced fMRI activations and higher order ISFCs is almost uniform, indicating their equal contribution to the enrichment of stimulus-related information and the improvement in the decoding capability of the dataset. Additionally, we noticed that the optimal weight of the first order ISFC was much higher than the rest of the levels, suggesting that a greater amount of stimulus-driven brain dynamics information is contained in this level. The significance of the first order ISFC in the mixture model is also consistent with high resolution results in HOBD ILDA, where first level ISFC achieved the highest decoding accuracy among all levels.

Like in HOBD ILDA, we were curious if changing TimeCorr Gaussian variance (changing temporal resolution loss across levels) would influencing the results, therefore we repeated the Multi-Level Mixture Analysis on each dataset with multiple TimeCorr setups. However, the optimal weight distribution qualitatively remained the same in every implementation: the first level ISFC always possesses the highest weight, while other levels always possesses similar weight. This further confirms that, although PCA reduced fMRI activations may sometimes produce higher decoding accuracy at low resolutions, first order ISFC actually contains the most information on brain dynamics that are stimulus-related.


\clearpage
\newpage
\section{Conclusion}
We proposed the High-Order Brain Dynamics Model, a new way to understand brain dynamics by incorporating information on high-order inter-subject functional connectivity into our analysis. In the process, we proposed the TimeCorr method, a lossless alternative to the sliding window approach for functional connectivity calculation, and the TimeCorr Level Up method, a new means of calculating high-order functional connectivity. Through testing on synthetic data, we demonstrated the superiority of the TimeCorr approach over the sliding window approach in recovering temporal functional connectivity, as well as its ability to differentiate between datasets with different correlation dynamics (block vs ramping). With TimeCorr and Level Up, we had the means to peer into the secrets of the brain through the lens of the High-Order Brain Dynamics Model. First, we calculated 10 orders of functional connectivity from the fMRI data of 3 past studies (Pieman, Forrest and Sherlock). Then, we carried out decoding analysis individually for each order of functional connectivity to understand how well stimulus-driven brain dynamics information is represented at each level. Afterwards, we conducted the Multi-Level Mixture Analysis to understand how information at each order of functional connectivity complements each other. Finally, as the decoding accuracy for all three datasets across all levels were generally better for TimeCorr implementations with Gaussian variance of 10 time points instead of $\text{min}(1000,\text{dataset time length})$ time points, we extrapolated from our synthetic dataset testing results that the brain activities in the Pieman, Sherlock and Forrest datasets may consist of multiple different length time blocks, each possessing stable correlation structures within its scope. Christopher Baldassano and colleagues confirmed this extrapolation for the Pieman and the Sherlock datasets in one of their recent papers \citep{Baldassano2016}.

Through the lens of the High-Order Brain Dynamics Model, we were able analyze the patterns in higher orders of functional connectivity and achieve a better understanding of stimulus-driven brain dynamics in existing datasets (Pieman, Forrest and Sherlock). In addition, through the construction of the Multi-Level Mixture Model, we were able to find effective ways to optimally combine stimulus-relevant information from multiple orders of functional connectivities into one functional connectivity matrix. This process enriches the amount of stimulus-relevant information in existing dataset without creating redundancies, which greatly increases the brain decoding capabilities of these dataset and has practical applications in many areas.

Firstly, this technology can potentially be used for mental evaluations in medical diagnosis and criminal investigations. After collecting the cognitive responses to certain stimuli of normal subjects, we can use the HOBD model to create a more comprehensive representation of their brain dynamics using information from high-order functional connectivity, as well as calculate the average range of decoding accuracies of the normal population. Then, we can use the HOBD model to create a similar representation of the cognitive response of any new subjects to the same stimuli. By using the decoding analysis to cross validate the high-order functional connectivity representation of the subject's brain responses with the ISFC of normal subjects, we can compare the resulting decoding accuracy with the previously calculated range for normal subjects to detect any cognitive anomalies in the subject's responses. This will help us identify any potential psychological or mental disorders, or suspicious reactions to crime scene reenactments. Although similar technologies already exist, we believe by constructing a more complex but comprehensive representation of high-order brain activities, we are inherently using more criteria for classification and are thus able obtain higher classification accuracy.

Secondly, as the HOBD model is able to generate more stimulus-relevant information from brain activities through analysis of high-order functional connectivities, we are able to produce more feature points to represent the state of the brain at each time point. Having a more complex feature map and a more comprehensive representation of the underlying temporal activities of the brain will increase the ability of Machine Learning algorithms to learn the patterns and relationships between the stimulus and the corresponding temporal brain activities. The trained Machine Learning models can then be used to recreate stimuli (visual, audio, etc) from HOBD representations of new brain activities, which could potentially give us more knowledge on human perception patterns.

Although we were able to increase the amount of stimulus-driven brain dynamics information retrievable from fMRI datasets through the use of TimeCorr and TimeCorr Level Up, we believe the abrupt decrease in decoding accuracy following the second order ISFC does not accurately represent the amount of information contained in these levels and is indicative of information loss. As our tests proved that TimeCorr is not the only cause for the decrease in decoding accuracy, we hypothesize that the information loss may also have been partially caused by PCA dimensionality reduction in the TimeCorr Level Up processes.

To address this issue and to further improve the High-Order Brain Dynamics Model in the future, we intend to experiment with the following options:

Firstly, we now have more confidence that the brain activities of subjects in the Pieman, Forrest and Sherlock datasets on average mainly consists of time blocks of stable correlation structures. Therefore, to better calculate the temporal functional connectivity at each time point, using only activations of time points within the scope of the corresponding correlation time block could significantly improve the accuracy of our calculations. One difficulty to this approach is the detection of block borders where the abrupt correlation changes occur. Christopher Baldassano and colleagues have tried using a variant of the Hidden Markov Model to detect the transitions between time blocks and achieved good results \citep{Baldassano2016}. We believe implementing this approach into our HOBD model could very likely improve the quality of our functional connectivity recoveries.

Secondly, we plan to experiment with other dimensionality reduction algorithms, like Spacial ICA. Although PCA is able to efficiently combine brain voxels with similar activity patterns and effectively preserve the general correlation dynamics within a dataset, it loses considerable amounts of important information by neglecting spatial distinctions between voxels. The correlation between voxels with similar activity patterns may be disposable when the voxels are from the same brain region, but the correlation between similar voxels across different brain regions could potentially contain very important information on the stimulus-driven dynamics of the brain. Alternative methods like spacial ICA are not only able to effectively reduce dataset dimensionality like the PCA method, but also does so while taking into consideration the spacial distinction between voxels. Spacial ICA has seen applications in multiple projects and has achieved good results \citep{Dipasquale2015,Iraji2016,Xu2013}, so we believe it could potentially be an effective alternative to PCA in our HOBD model.

-------------------MOVED HERE-------------------

Thus, we hypothesize that exploration of high-order functional connectivities within the brain---the correlation structure within functional connectivities---could potentially reveal more about the dynamics behind brain functions. To achieve this, we present the High-Order Brain Dynamics (HOBD) model, which examines and incorporates information from multiple orders of brain functional connectivities to gain a more comprehensive understanding of how the brain responds to controlled stimuli. The HOBD model also introduces two new methods: TimeCorr, which revolutionizes the way temporal functional connectivity is calculated, and TimeCorr Level Up, a new scalable solution to retrieving functional connectivities at high orders. Through the lens of the the HOBD model, we gained a better understanding of how the dynamics of stimulus-driven brain activities are represented by different levels of functional connectivities, and how different orders of functional connectivities complement each other in providing non-overlapping information on stimulus-driven brain patterns.

Lastly, \textbf{important information may exist beyond voxel activations and their first level functional connectivities.} The brain is a network of seemingly autonomous nodes that are actually intricately interconnected. To fully grasp the functions of a single brain region in the network, it is crucial to understand its activities in the context of the richly woven network that supports it \citep{Battiston2017}. As demonstrated by previous efforts by the Contextual Dynamics Lab, high level functional connectivities may contain non-overlapping cognitive information inaccessible by functional connectivities at lower levels \citep{jeremy2017}. For example, useful information about the brain may exist within interactions between interactions between brain regions (2nd order functional connectivity), interactions between interactions between interactions (3rd order functional connectivity), or in even higher orders of functional connectivity. Therefore, incorporating information from higher order dynamics could potentially improve the quality of brain analyses.

Secondly, we designed a ``level up" method that utilizes TimeCorr, Inter-Subject Functional Connectivity (ISFC) and Principle Component Analysis (PCA) to calculate higher order functional connectivity. As illustrated in Figure \ref{fig:HighOrder}, the process of leveling up involves repeatedly finding the correlation of correlation structures. Intuitively, voxels in the brain are analogous to the intersections within a 3-dimensional web: all voxels are delicately connected with each other, forming small sections that interweave into larger and larger clusters. Similarly, when we find the temporal correlation between two voxels, we gain information on how they interact with each other at that specific time point. By ``leveling up", we find the correlation between the interactions of two pairs of voxels, which gives us additional information on the dynamics of a slightly larger community (4 voxels). Following this intuition, every time a new order of functional connectivity is calculated on top of another level of functional connectivity, we are essentially retrieving information on the dynamic relationship between increasingly larger clusters, which could potentially be useful for achieving a better understanding of the network dynamics inside our brains.

-------------------END MOVE----------------------

To conclude, recovering high-order functional connectivity patterns is a challenging task, but is also crucial for achieving a more in-depth and comprehensive understanding of stimulus-driven brain dynamics. We introduced the HOBD model as a new means to examine and incorporate useful information from high-order functional connectivities into our analysis. Using this model, we were able to discover that important information for decoding the brain is contained within these latent structures. We believe the applications and benefits of high-order brain connectivities are endless, and we are excited to explore new approaches to improve the HOBD model and expand the horizon of human brain decoding.

\newpage
\bibliographystyle{apalike}
\bibliography{HOBD}
\end{document}
