\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage[vlined, ruled]{algorithm2e}
\usepackage{geometry}
\usepackage[noend]{algpseudocode}
\usetikzlibrary{bayesnet}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{enumitem,kantlipsum}
\usepackage{float}
\usepackage{titlesec}
\usepackage{natbib}
\usepackage{apalike}
\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\setlength{\parskip}{1em}
\geometry{letterpaper,left=1.5in,right=1in,top=1in,bottom=1in}
\setlength\parindent{0pt}
\linespread{1.5}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\tr}{tr}

\begin{document}
\pagenumbering{gobble}
{\centering
  \textbf{The High-Order Brain Dynamics (HOBD) Model}\par
  A Thesis\par
  Submitted to the Faculty\par
  in partial fulfillment of the requirements for the\par
  degree of\par
  Master of Science\par
  in\par
  Computer Science\par
  by\par
  Hao Chang\par
  DARTMOUTH COLLEGE\par
  Hanover, New Hampshire\par
  May, 2017\par
}
\vspace{5mm}
\setlength{\parskip}{0em}
\begin{flushright}
Examining Committee:\par
\vspace{5mm}
$\rule{5cm}{0.15mm}$\par
Hany Farid\par
\vspace{5mm}
$\rule{5cm}{0.15mm}$\par
Qiang Liu\par
\vspace{5mm}
$\rule{5cm}{0.25mm}$\par
Jeremy Manning\par
\end{flushright}
\setlength{\parskip}{1em}
\newpage


\null\par
\newpage
\pagenumbering{roman}
\section{Abstract}
Recent researches focused on analysis of brain functional connectivity---pair-wise correlation between time series of activities in different brain regions---has achieved monumental breakthroughs in improving our understanding of the human brain. Moreover, new efforts have attempted to cast functional connectivity as a dynamic process that changes from moment to moment, and have revealed that important brain dynamics information exists within these dynamic correlation structures. Thus, we hypothesize that exploration of high-order functional connectivities within the brain---the correlation structure within functional connectivities---could potentially reveal more information on the dynamics behind brain functions. To achieve this, we present the High-Order Brain Dynamics (HOBD) model, which incorporates information from multiple orders of brain functional connectivities to gain a more comprehensive understanding of how the brain responds to controlled stimuli. The HOBD model also introduces two new methods: TimeCorr, which revolutionizes the way temporal functional connectivity is calculated, and TimeCorr Level Up, a new scalable solution to retrieving functional connectivities at high orders. Through the lens of the the HOBD model, we gained a better understanding of how different levels of functional connectivities represent stimulus-driven brain dynamics information, and how functional connectivities at different orders can complement each other to provide more information on stimulus-driven brain patterns.

\newpage
\section{Acknowledgements}
Firstly, I would like to thank my mentors Professor Jeremy Manning of the Psychological and Brain Sciences Department at Dartmouth College and Professor Qiang Liu of the Computer Science Department at Dartmouth College. Professor Manning and Professor Liu were great inspirations for me throughout this project. Their deep understanding of their respective fields and constant flow of bright ideas really expanded my understanding of what it means to love and excel at what I want to do. Professor Manning and Professor Liu provided timely guidance whenever I was in need and constantly encouraged me to try out new ideas and reach for higher standards.

Secondly, I would like to thank Professor Hany Farid of the Computer Science Department at Dartmouth College. Professor Farid kindled my first spark of interest in Computer Science and pushed me to pursue graduate education at Dartmouth. He has been a role model for me throughout my time at Dartmouth and will always be an inspiration for me in my future pursuits in life.

Thirdly, I would like to thank Andy Heusser and Kirsten Ziman in the Contextual Dynamics Laboratory in the Neuroscience Department at Dartmouth College. Andy and Kirsten played crucial roles in helping me overcome my lack of experience in neuroscience related researches. Their patient and timely assistance helped me through many difficulties at crucial stages of my research.

Fourthly, I would like to thank Dilin Wang, Jun Han and Yihao Feng in the DartML Laboratory in the Computer Science Department at Dartmouth College. Their door were always open whenever I ran into a difficult spot or had questions about my research or writing.

Finally, I must express my very profound gratitude to my parents for providing me with unfailing support and continuous encouragement throughout my years of study and through the process of researching and writing this thesis. This accomplishment would not have been possible without them. Thank you.

\newpage
\tableofcontents
\newpage
\pagenumbering{arabic}

\section{Introduction}
Human beings have an insatiable desire to understand, not only the things around us, but also ourselves. Why do we behave and respond the way we do \citep{hasson2012}? How do we learn and adapt \citep{hasson2004}\citep{hasson2005}? What do we believe or value \citep{Greene01}? How does social context affect the way we think \citep{Matthew2015}? These questions have mesmerized wise men for thousands of years, but the answers continue to evade us. For many, the key to understanding human behavior lies in our brain, the central hub of all of our thoughts and decision making. So, when a new technology emerged and allowed us to study the brain in a quantitative manner, it opened a door of endless possibilities for those who possessed an aptitude for quantitative analysis and a hunger to learn more about our own identity.

Functional Magnetic Resonance Imaging, often referred to as fMRI, was first discovered and applied as a brain mapping method in 1990 by Seiji Ogawa \citep{Ogawa90}. This technology then quickly popularized among Brain Science related researches due to its unprecedented low health risk to subjects and its ability to accurately translate brain activities into highly-structured quantitative data \citep{Logothetis01}\citep{Friston99}\citep{Friston98}. Once highly abstract brain activities are converted into signs and digits, exploring patterns in the brain simply devolves into highly intricate mathematical problems. For example, by treating each brain fMRI image as a feature vector, Machine Learning algorithms trained on a subset of the data can learn to identify cognitive patterns (e.g. which part of a movie someone is watching), and be used to extract similar patterns from held-out brain images \citep{Norman06}\citep{peterson12}\citep{peterson17}. These applications typically treat each brain image in isolation, and attempt to decode patterns of activities associated with each of several candidate brain states. However, important stimulus-driven activities in fMRI data are often obscured by noises created by non stimulus-driven neural activities \citep{peterson11}, which greatly reduces the effectiveness of any effort to analyze relevant brain activities \citep{hasson2009}. To address this issue, new branches of research have been emerging that specifically focus on the extraction of useful information from noisy brain fMRI data by conducting a time series analysis incorporating information from the entire dataset \citep{tang2017}, and one of the most fruitful directions involves analyzing functional connectivities within the brain \citep{peterson19} \citep{peterson20}.

The brain can be divided in various ways---according different rules of systemization (by functionality, organization, activity patterns, etc)---into a set of regions, and functional connectivity describes the correlation between the time series of activations each pair of brain regions exhibits. When we observe the brain, the neural activities can appear incredibly complex. But the mechanics of the brain are far from random, rather, the correlation patterns our brains exhibit are highly structured. Presumably, this mirrors the complex but highly structured nature of our internal thoughts and experiences. Many past studies on functional connectivity have focused on using resting state data (fMRI images taken as participants rest in the scanner with their eyes open) to estimate a single correlation matrix for each person, which provides a rich database for mapping the relations among brain processes and their contributions to perception, action and cognition \citep{peterson9}\citep{Bassett2017}. The cognitive information of each subject was then used as identifiers to compare how the relationship between brain activity and task behavior differ or agree between subjects \citep{Turke13}\citep{Rubinov2010}\citep{peterson10}.

Recent research efforts have revealed that important cognitive information is also contained in the temporal correlational structure of time series brain activities \citep{davidson2016}, and as a result, approaches focusing on understanding the brain through analysis of dynamic brain functional connectivities begun to rapidly emerge \citep{Nigam2016}\citep{Hutchinson2013}. One of the most significant advancements comes from a recent paper by Uri Hasson's group, which casts functional connectivity as a dynamic process that changes from moment to moment depending on the specific experience of that moment \citep{hasson2016}. When analyzing the dynamic functional connectivities of one experimental subject in isolation, one can examine how (or whether) the correlational structure of these activity patterns (across a given set of brain regions) varies according to the cognitive task the subject is performing. The paper also presents the revolutionary Inter-Subject Functional Correlation (ISFC) analysis, which cross-examines functional connectivities across multiple subjects to isolate stimulus-dependent inter-regional correlations from noises created by random neural activities. The Contextual Dynamics Lab successfully applied dynamic functional connectivity and ISFC analyses in their most recent paper on Hierarchical Topographic Factor Analysis (HTFA), and demonstrated that functional connectivity patterns and basic brain activation patterns contain partially non-overlapping information on stimulus-driven brain dynamics \citep{jeremy2017}. This discovery was the main inspiration for the high-order functional connectivity analysis in this thesis.

Although functional connectivity analyses of brain data has evolved significantly \citep{olaf2005}\citep{khambhati2017}, there are still three fundamental limitations to past approaches:

Firstly, \textbf{correlation matrices are not scalable.} Examining pairwise correlations in brain data containing $n$ regions (henceforth interchangeable with ``voxels") produces a correlation matrix with $n^2$ entries. When n is large (e.g. the number of brain regions in an fMRI volume), the full correlation matrix can become unwieldy for computation \citep{Rubinov2010}\citep{Betzel2017}\citep{Craddock2012}\citep{Yeo2011}. Furthermore, if one wishes to examine higher-order patterns (e.g. how correlations between correlations change over time), the storage requirements increase exponentially.

Secondly, \textbf{the sliding window approach is not well suited for the study of dynamic activity.} Many past approaches to calculating dynamic functional connectivities use the sliding window method, where the correlation function is applied on the brain activations of time points within a window of set length to calculate the functional connectivity at the central time point. To calculate the functional connectivities of following time points, the window is shift forward one time point at a time until the edge of the window reaches the end of the time series \citep{enrico2011}\citep{elena2012}. One disadvantage of the sliding window approach is that, for a time series of length $t$ and a window length of $l$, it can only retrieve the functional connectivities of $n=t-l$ time points, thereby losing information on $l$ time points. Although many practical applications use buffers to make up for the loss, repeated application of the sliding window approach on the same dataset—--e.g. calculating the correlations of the correlations between nodes—--is impractical. In addition, due to the limitations on window length, the sliding window approach lacks the flexibility to adjust between different temporal resolutions. In many situations, sliding window implementations with insufficient window length provide only a poor approximation of the true moment-by-moment dynamic correlation patterns.

Lastly, \textbf{important information may exist beyond voxel activations and their first level functional connectivities.} The brain is a network of seemingly autonomous nodes that are actually intricately interconnected. To fully grasp the functions of a single brain region in the network, it is crucial to understand its activities in the context of the richly woven network that supports it \citep{Battiston2017}. As demonstrated by previous efforts by the Contextual Dynamics Lab, high level functional connectivities may contain non-overlapping cognitive information inaccessible by functional connectivities at lower levels \citep{jeremy2017}. For example, useful information about the brain may exist within interactions between interactions between brain regions (2nd order functional connectivity), interactions between interactions between interactions (3rd order functional connectivity), or in even higher orders of functional connectivity. Therefore, incorporating information from higher order dynamics could potentially improve the quality of brain analyses.

Here, we present the High-Order Brain Dynamics (HOBD) model, a multilayer perspective that seeks to achieve a deeper understanding of the brain by finding new ways of representing high-order brain interactions and analyzing the dynamics behind these different ``levels" of brain interactions. To efficiently access and analyze brain dynamics at higher levels, the HOBD model revolutionizes functional connectivity calculation methods and creates a means to repeatedly ``level up" brain dynamics information---computing high level functional connectivities from low level information. These changes have notably improved correlation recovery accuracy, and significantly increased the amount of useful information we can extract from existing brain fMRI datasets.

Firstly, to find a more effective method to calculate temporal functional connectivity, we designed TimeCorr, an intuitive method that finds its inspiration directly from the fundamental correlation function. Like the sliding window method, TimeCorr is able to recover functional connectivity from fMRI brain images with similar if not higher accuracy. But TimeCorr goes beyond the sliding window method by (a) not requiring buffers for its calculations, thereby avoiding data loss, (b) offering extra stability by using all time points in the time series for functional connectivity calculation at every time point, (c) and providing the option to balance the locality and stability of its results through adjusting its temporal resolution parameter.

To achieve the functionalities mentioned above, TimeCorr makes use of the Gaussian distribution. By attaching a Gaussian coefficient to components at each time point in the time series, TimeCorr is able to allocate the amount of influence each neighboring time point has on the calculation of the correlation at the Guassian center. As the Gaussian center (highest density/coefficient) is always at the time point of interest, TimeCorr guarantees to return highly accurate approximations of the temporal ground truth. In addition, the user can choose to have more stability in the results by choosing coefficients from a Gaussian distribution with a larger variance; or higher temporal resolution and more locality by choosing coefficients from a distribution with a smaller variance.

Secondly, we designed a ``level up" method that utilizes TimeCorr, Inter-Subject Functional Connectivity (ISFC) and Principle Component Analysis (PCA) to calculate higher order functional connectivity. As illustrated in Figure \ref{fig:HighOrder}, the process of leveling up involves repeatedly finding the correlation of correlation structures. Intuitively, voxels in the brain are analogous to the intersections within a 3-dimensional web: all voxels are delicately connected with each other, forming small sections that interweave into larger and larger clusters. Similarly, when we find the temporal correlation between two voxels, we gain information on how they interact with each other at that specific time point. By ``leveling up", we find the correlation between the interactions of two pairs of voxels, which gives us additional information on the dynamics of a slightly larger community (4 voxels). Following this intuition, every time a new order of functional connectivity is calculated on top of another level of functional connectivity, we are essentially retrieving information on the dynamic relationship between increasingly larger clusters, which could potentially be useful for achieving a better understanding of the network dynamics inside our brains.

\begin{figure}[!htb]
\setlength{\belowcaptionskip}{15pt}
\caption{The Level Up Process}
\centering
\includegraphics[width=1\textwidth]{../figures/High-Order.png}
\label{fig:HighOrder}
\end{figure}

To overcome the problem of scaling and to enforce dimension uniformity between levels for inter-level analysis, we use TimeCorr to calculate higher order functional connectivities from lower level activations, and then reduce the TimeCorr results to an arbitrary number of voxels to represent the activations for the new level. As TimeCorr avoids data loss and PCA maintains an uniform number of activations after each ``level up", we are able to obtain functional connectivities up to the $10^{th}$ order while ensuring linear scaling in storage and computation. In addition, as PCA achieves dimensionality reduction by combining voxels with similar activity patterns, the TimeCorr Level Up function only generates correlations between highly distinct regions, thus producing more informative functional connectivities. However, the cost of dimensionality reduction is that the connectivity matrices are not represented with perfect fidelity, so there will necessarily be information loss at each ``level up".

As mentioned previously, we believe different orders of functional connectivity represent information on different aspects of brain dynamics. To measure the amount of information present at each level, we introduce decoding accuracy, a testing parameter that describes the proportion of time points in a subject's functional connectivity time series that have highest correlation with the same time point in the ISFC of all other subjects. Time points that share high similarity across all subjects have high probability of being stimulus-driven, therefore the decoding accuracy is a good evaluation of the average proportion of brain activities that are relevant to the stimulus. Furthermore, as we lack knowledge of the true functional connectivity structures of real fMRI datasets, the decoding analysis also serves as a good alternative to assess the quality of our recovery efforts. We are interested in how decoding accuracy changes as we reach higher order functional connectivity. Although we presumed brain dynamics of different subjects would converge as we get to higher orders, resulting in increasing decoding accuracy, reality proved otherwise. This is shown and discussed in more detail in the Intra-Level Decoding Analysis section of our results.

Finally, to carry out an in-depth evaluation of the effectiveness of the HOBD model, we also conducted the Multi-Level Mixture Analysis. This study aims to understand how the dynamic information from different levels of brain connectivity can be used in unison (weighted sum) to improve the decoding capabilities of existing brain fMRI datasets. In the Multi-Level Mixture Analysis section of our results, we provide a side by side comparison that highlights the improvements achieved by our mixture model. In the process of finding the mixture model that maximizes decoding accuracy, we also analyzed the optimal distribution of weights for each level of brain dynamics to explore their contribution to the optimal mixture model. The methods and findings will be discussed extensively in the Results section.

\newpage
\section{Model Description}
The High-Order Brain Dynamics model seeks a new perspective to understand the dynamics behind stimulus-driven brain activities. Utilizing the capabilities and advantages of TimeCorr and TimeCorr Level Up, the HOBD model attempts to incorporate information on high-order functional connectivities into its analysis of brain dynamics.

\subsection{Single Subject TimeCorr}
The TimeCorr method was designed for the purpose of replacing the sliding window method as a lossless alternative to achieve more accurate calculations of dynamic functional connectivities (the temporal correlation at each time point between pairs of brain regions) from brain fMRI data. The sliding window method operates by applying the correlation calculation function on the activations of time points within a window of length $L_w$ and centering on the time point of interest $t$, where $L_w$ must be of considerable magnitude for the sliding window method to achieve respectable accuracy. However, due to its inherent design flaw, using the sliding window method on a dataset of time length $T$ could only yield functional connectivities for $T-L_w$ data points, a loss not insignificant due to the typical magnitude of $L_w$. Thus, as calculating high-order functional connectivities involves repeated application of the method of correlation calculation on the dataset, using the sliding window method would result in rapid deterioration in the amount of usable data. In addition, as the sliding window method places equal weights on every time point within its window of calculation, it can only achieve a rough approximation of the average correlation within the window but not the instantaneous truth at each time point.

To solve these problems, we designed the TimeCorr method, which finds inspiration from the fundamental correlation function. Instead of isolating only a block of time points, TimeCorr effectively utilizes information from the entire dataset for functional connectivity calculation at every time point. To ensure locality of TimeCorr's results, the components at each time point are multiplied by a weight drawn from a normalized Gaussian density function so that the influence of each time point on the calculation of functional connectivity at time point $t$ is proportional to its distance from $t$. This adaptation of the correlation function effectively eliminates the need to sacrifice buffer time points at the beginning and end of the time series, thereby increasing the number of output time points to equal to the number of input time points and avoiding any data loss. In addition, through the application of Gaussian density coefficients, TimeCorr calculations place more emphasis on information closer to the time point of interest, thereby ensuring a more accurate estimation of the temporal ground truth.

\large{\textbf{Formal Definition}}

\normalsize
Given a single subject's brain activation matrix with T time points and V voxels, to apply TimeCorr:

\begin{enumerate}
\item Base on the amount of influence neighboring time points should have on the functional connectivity calculation at each time point, choose an appropriate Gaussian variance $\sigma$ to generate coefficients.

\item For each time point $t$:
\begin{enumerate}
\item Using a Gaussian density function of variance $\sigma$ and mean $t$, generate an array of coefficients $w_t$ of length T, where each element of $w_t$ can be calculated through the following equation:
\begin{align*}
w_x = \frac{1}{\sqrt{2\sigma\pi}}e^{-\frac12 \frac{(x-t)^2}{\sigma}}
\end{align*}

\item For each voxel $i$, element-wise multiply its activation time sequence $a_i$ by the coefficients array to create a weighted activations array $S^i_t$.

\item Find the temporal correlation between voxel i and voxel j at time point t through the equation:

\begin{align*}
C(S^i_t,S^j_t) = \frac{1}{Z}\frac{\sum_{l=0}^T (S_l^i - \bar{S^i_t})\cdot(S^j_l - \bar{S^j_t})\cdot \mathcal{N}(l|t,\sigma)}{\sigma_{S_t^i} \cdot \sigma_{S_t^j}}
\end{align*}
Where
\begin{align*}
Z &= \sum_{l=0}^T \mathcal{N}(l|t,\sigma)\\
\bar{S^i_t} &=\frac{1}{Z} \sum_{l=0}^T S^i_l \cdot \mathcal{N}(l|t,\sigma)\\
\sigma_{S_t^i} &=\sqrt{ \frac{1}{Z}\sum_{l=0}^T (S_l^i-\bar{S_t^i})^2 \cdot \mathcal{N}(l|t,\sigma)}\\
\end{align*}
\item Repeat the process above for every voxel pair to create a correlation matrix for time point $t$
\end{enumerate}
\item Convert the correlation matrix at every time point to its inverse squareform format (vectorized array containing half of the non-diagonal elements of a square matrix), outputting an array of size $T$ by $(V^2-V)/2$ dimensional matrix, where $T$ and $V$ represent the time length and voxel count of the dataset.
\end{enumerate}

The Gaussian variance parameter V gives the user extra freedom to customize the desired level of resolution for temporal correlation calculation. If a large variance is chosen, the influence of neighboring time points on the functional connectivity calculation at each time point is increased, thereby adding smoothness and stability to the resulting functional connectivity time sequence and providing a more accurate representation of the general trend. But if a small variance is chosen, most of the weight will be placed on the time point of interest and its immediate neighbors, so the resulting functional connectivity time sequence will have more emphasis on local accuracy. After experimenting with different setups, we discovered that choosing a Gaussian variance equal to the minimum between 1000 and the total number of time points achieves a good balance between locality and fluidity. This finding will be discussed in more detail in the Results section.

In contrast to the sliding window method, the TimeCorr approach is able to more accurately retrieve the temporal correlation at each time point without loss of important data. This advantage makes it possible to ``level up" to higher order functional connectivities---progressively finding the correlation of correlations---and explore dynamics and patterns heretofore untouched. We will go into more detail about the ``level up" process and high-order functional connectivity in the ``Multi-Subject TimeCorr Level Up" section.

\subsection{TimeCorr Inter-Subject Functional Connectivity}

Inter-Subject Functional Connectivity (ISFC) is the functional connectivity between brain regions of different subjects \citep{jeremy2017}\citep{hasson2016}. The patterns recovered by ISFC is analogous to single subject functional connectivities (which reflect the correlational structure across brain regions within one individual's
brain), but they should reflect only activities that are specifically stimulus-driven. For every subject $i$, we compare its brain activations time sequence $a_i$ with the average activations of all other subjects $a_{-i}$. Through the process of averaging the activations of multiple subjects, we dampen random noises and emphasize activities that are common across all subjects, which are usually stimulus-dependent. Therefore, when we calculate the functional connectivity matrix between subject activation $a_i$ and the average response from the rest of the participants $a_{-i}$, the functional connectivities we calculate are more likely to reflect activities that are stimulus-driven.

Furthermore, after we obtain the functional connectivity matrix between each subject and the average of their counterparts, we use Fisher Z-transformation to average the results from all the subjects. In addition to the noise-dampening and stimulus-enhancing effects that comes with averaging, we are also able to gain an unbiased view of the overall stimulus-driven functional connectivity patterns across all subjects within our analysis. Fisher Z-transformation is applied in the averaging process as a means to stabilize and reduce approximation variance and return a less biased result than that of directly averaging correlations.

\large{\textbf{Formal Definition}}

\normalsize
Given an activations dataset containing $N$ subjects, each with T time points and V voxels, to apply TimeCorr ISFC:
\begin{enumerate}
\item Determine the desired temporal resolution for calculation and select $\sigma$, the variance of the Gaussian density function for TimeCorr coefficients generation.
\item For each subject s,
\begin{enumerate}
\item Find the average activation of all other subjects:
\begin{align*}
O_s=\frac{\sum_{i\neq s}^N S_i}{N-1}
\end{align*}
where $S_i$ represents the activation matrix for subject $i$, and $N$ represents the total number of subjects.
\item Find the functional connectivity between the brain activations time series $S$ for subject $s$ and the average activations of all other subjects $O$ using TimeCorr ISFC with variance $\sigma$. The correlation between $S^i_t$---activation of voxel i for subject $s$ at time $t$---and $O^j_t$---average activation of voxel $j$ of all other subjects at time points $t$---is obtained through the following equation:
\begin{align*}
C(S^i_t,O^j_t) = \frac{1}{Z}\frac{\sum_{l=0}^T (S_l^i - \bar{S^i_t})\cdot(O^j_l - \bar{O^j_t})\cdot \mathcal{N}(l|t,\sigma)}{\sigma_{S_t^i} \cdot \sigma_{O_t^j}}
\end{align*}
Where
\begin{align*}
Z &= \sum_{l=0}^T \mathcal{N}(l|t,\sigma)\\
\bar{S^i_t} &=\frac{1}{Z} \sum_{l=0}^T S^i_l \cdot \mathcal{N}(l|t,\sigma)\\
\bar{O^i_t} &=\frac{1}{Z} \sum_{l=0}^T O^i_l \cdot \mathcal{N}(l|t,\sigma)\\
\sigma_{S_t^i} &=\sqrt{ \frac{1}{Z}\sum_{l=0}^T (S_l^i-\bar{S_t^i})^2 \cdot \mathcal{N}(l|t,\sigma)}\\
\sigma_{O_t^i} &=\sqrt{ \frac{1}{Z}\sum_{l=0}^T (O_l^i-\bar{O_t^i})^2 \cdot \mathcal{N}(l|t,\sigma)}\\
\end{align*}
\item Repeat the above process for every pair of voxels, for every time point to obtain a time series of correlation matrices
\end{enumerate}
\item Element-wise apply Fisher Z-transformation (r2z) to the correlation matrices for each subject at each time points to obtain the corresponding Z-correlation matrices:
\begin{align*}
r2z(r) = \frac{1}{2}\ln(\frac{1+r}{1-r})
\end{align*}
\item Element-wise average the Z-correlation matrices $Z_i$ across all subjects:
\begin{align*}
S_Z = \frac{1}{N}\sum^N_{i=1}Z_i
\end{align*}
\item Element-wise apply inverse Z-transformation (z2r) to the average Z-correlation matrix to obtain the TimeCorr ISFC matrix:
\begin{align*}
\text{TimeCorr ISFC} = z2r(S_Z) = \frac{\exp(S_Z+S_Z^T)-1}{\exp(S_Z+S_Z^T)+1}
\end{align*}
\item Convert the correlation matrix at every time point to its inverse squareform format, and output a $T$ by $(V^2-V)/2$ dimensional matrix containing the average functional connectivity of all subjects in the dataset, where $T$ and $V$ represent the time length and voxel count of the dataset, respectively.
\end{enumerate}

\subsection{Multi-Subject TimeCorr Level Up}
The multi-subject TimeCorr Level Up method utilizes the functionalities of the single-subject TimeCorr method to extract high-order brain dynamics patterns from brain fMRI data. Due to the data-loss flaw of the sliding window method, researches related to functional connectivity heretofore have been largely limited to analysis of raw fMRI activations and first order functional connectivity. However, with the availability of TimeCorr, we are able to repeatedly find higher order functional connectivities of an fMRI dataset without fear of losing data with each ``level up". This advantage allows us to explore higher order dynamic patterns within the brain that were previously impossible to access.

The concept of ``leveling up" involves repeatedly finding the correlation of correlations of brain activity. Given a brain fRMI dataset, we designate the raw activations as level 0 and the functional connectivity (correlation structure) of the raw activations as level 1, the result of ``leveling up" from level 0 (henceforth ``level" and``order" are interchangeable). Hereafter, given a matrix containing the functional connectivities of the $n^{th}$ order, we can use the TimeCorr Level Up method to find its dynamic correlation structure and obtain the $n+1^{th}$ order functional connectivities.

In addition to effectively retaining all relevant data, the Multi-Subject TimeCorr Level Up method also presents a scalable solution to large datasets. Normally, for an input dataset of $V$ voxels (brain regions), the first order functional connectivities would occupy $V^2$ space and every additional level of functional connectivities would involve an exponential increase in storage requirement. In order to circumvent this problem, the Multi-Subject TimeCorr Level Up method first finds the inverse square-form (vectorized array containing half of the non-diagonal elements of a square matrix) of the correlation matrices for every time point in the ``level up" output, and then uses PCA to further reduce the result to a matrix of $T$ by $V$ dimensions, where $T$ and $V$ represent the time length and voxel count of the input dataset. This solution ensures all orders of functional connectivities derived from the same dataset will have the same dimensions. In addition, the application of PCA further guarantees that only the principle components of each functional connectivity matrix is retained for further analysis.

\large{\textbf{Formal Definition}}

\normalsize
Given a brain activation matrix or a $n^{th}$ order functional connectivity matrix containing N subjects, V voxels and T time points, the Level Up function

\begin{enumerate}
\item Applies single-subject TimeCorr on the activation matrix of each subject to obtain their corresponding functional connectivity matrices

\item Concatenates the correlations matrix for each subject from the previous step together along the first (time) dimension into a single matrix of dimensions $(N*T)$ by $(V^2-V)/2$

\item Applies PCA on the concatenated matrix from the previous step to obtain a reduced representation of the correlation matrix of dimensions $(N*T)$ by $V$

\item Separates the reduced correlation matrix from the previous step into data for each subject to obtain a 3-D matrix of size $N$ by $T$ by $V$, where $N$, $T$ and $V$ represent subject count, dataset time length and voxel count, respectively

\item Repeats the above process on the output to level up again

\end{enumerate}

By concatenating the functional connectivity of all subjects into a single matrix before PCA reduction, we guarantee consistency in the orientation of principal components in the reduced functional connectivity matrix across all subjects. As a result, although the original voxel order is disrupted in the reduction process, we are able to maintain activation alignment across subjects and preserve an effective representation of the dynamic correlation structure.

In addition, PCA reduction of brain activities follows division by similarity in activity. While regions with similar activity are combined, regions that display dissimilar patterns are isolated and highlighted. This process is inherently beneficial to functional connectivity analyses, as we are eliminating information redundancy from areas that display identical or similar activity, and placing our focus on areas that possess greater differences in functionality. Intuitively, this advantage also extends to higher levels, where interactions displaying similar relationship with the network are combined, resulting in highly informative correlation structures of very diverse networks.

\subsection{Multi-Subject Decoding Analysis with TimeCorr ISFC}
The Multi-Subject Decoding Analysis is a method designed to find stimulus-dependent activations in an fMRI dataset by cross referencing data from multiple subjects \citep{jeremy2017}. Through application of Multi-Subject TimeCorr ISFC on two equal divisions of the subject data, non stimulus-driven neural activities are dampened and activities that are common across subjects are emphasized, thereby increasing the salience of patterns that reflect activities that are only stimulus-driven. Additionally, when the correlation between the same time point in both sub-groups is high, it indicates that the average brain activities in one group demonstrates very high similarity with the average brain activities of all subjects in the other group at that time point. When the brain activities of multiple subjects display high similarity for certain time points in their brain activations time sequence, then intuitively there's a high probability that their brain responses for those time points are uniformly caused by their common stimulus. Using this logic, the decoding accuracy of the dataset---the percentage of time points that demonstrates highest correlation with themselves across two sub-divisions of the dataset---gives an accurate representation of

\begin{enumerate}
\item Similarity between subject responses throughout the fMRI simulation
\item The average proportion of subject activities within the dataset that are stimulus dependent
\item The potency of subjects' brain response to the stimuli
\item The average proportion of subject activities that shows significant distinction from the rest of the time sequence
\item The potency of stimuli in generating salient cognitive response across all subjects
\end{enumerate}

Due to the above characteristics, the Decoding Analysis makes up a crucial part of our project as a means to evaluate the effectiveness of our newly developed methods.

\large{\textbf{Formal Definition}}

\normalsize
Given a multi-subject dataset containing $S$ subjects, each possessing $T$ time points and $V$ voxels, to conduct Decoding Analysis:
\begin{enumerate}
\item Select a Gaussian variance $\sigma$ for TimeCorr coefficient generation.
\item For n repetitions:
\begin{enumerate}
\item Randomly divide the subjects into two equal groups.
\item Calculate Inter-Subject Functional Connectivity (ISFC) for each group using TimeCorr. The resulting ISFC matrices are labelled $I_1$ and $I_2$.
\item Calculate time-point-wise correlation between functional connectivity at each time point in $I_1$ and functional connectivity at each time point in $I_2$, resulting in a $T$ by $T$ correlation matrix.
\item A time point $t$ in $I_1$ is correctly decoded if the corresponding highest correlation time point in $I_2$ is also $t$; and vice versa for decoding time points in $I_2$.
\item Count and record the number of time points that were correctly decoded in both $I_1$ and $I_2$.
\end{enumerate}
\item Sum the total number of time points correctly decoded in $I_1$ and $I_2$ across all repetitions, and find the average through dividing the sum by the product between the repetition count and two times the time point count (to account for both $I_1$ and $I_2$). The result is the decoding accuracy output of the multi-subject dataset:
\begin{align*}
\text{Decoding Accuracy} = \frac{\text{Total number of time points correctly decoded across all repetitions}}{\text{repetition count} * \text{time length} * 2}
\end{align*}
\end{enumerate}




\newpage
\section{Results}
\subsection{Testing Single Subject TimeCorr on Synthetic Datasets}
To conduct a side-by-side comparison of the correlation recovery functionalities between the TimeCorr method and the traditional sliding window method, we used Cholesky decomposition to construct synthetic datasets with pre-defined correlation patterns. This approach takes a random matrix X and an original correlation structure R, and uses the Cholesky transformation to create a new, correlated, Y matrix:
\begin{align*}
Y = Cholesky(R) \cdot X
\end{align*}
Using this approach, we are able generate a time sequence of synthetic brain activations that exactly follows a pre-designated a dynamically correlation structure.

Two kinds of synthetic datasets were generated to test different features of TimeCorr: (1) datasets divided into multiple time blocks, each containing different correlation structures, to test how TimeCorr performs when the correlation changes abruptly, (2) and datasets with linearly changing correlation structures to test how TimeCorr performs when the correlation changes gradually.

\subsubsection{Single Subject Synthetic Dataset with block correlations}

In datasets with block correlations, we divided the time sequence into $N$ blocks of equal time lengths. Each block possesses a different correlation that's consistent throughout the length of the block. These datasets will be used to test TimeCorr's ability handle abrupt correlation changes in a time series. To generate the block correlation dataset:
\begin{enumerate}
\item Using a random normal distribution with mean equal to 0 and variance equal to 1, generate a dataset X of dimensions $T$ by $V$, where $T$ and $V$ represents the desired time length and voxel (brain region) count, respectively.
\item Design a correlation matrix of dimensions $V$ by $V$ for each of the $N$ blocks, denoted $R_n$.
\item For each of the $N$ blocks in X, transform the block $X_n$ into correlated structure using the equation:
\begin{align*}
W_n = Cholesky(R_n) \cdot X_n
\end{align*}
Where $Cholesky(R_n)$ represents the Cholesky decomposition of the correlation matrix $R_n$
\item Piecing all of the blocks together to form a new synthetic dataset W that possesses the designated dynamic correlation structure.
\end{enumerate}

To evaluate and compare the performances of TimeCorr and the sliding window method in recovering the ground truth in block correlation datasets, we used four metrics: (1) correlation with the true correlation in each block, (2) Mean Square Error (MSE) with the true correlation in each block, (3) correlation with the temporal ground truth, and (4) MSE with the temporal ground truth. The first two metrics were chosen to demonstrate how well TimeCorr and the sliding window method can distinguish the true correlation within each block from the ground truth in other blocks. The last two metrics were chosen to demonstrate the overall performance of TimeCorr and the sliding window approach in recovering the temporal ground truth, especially during abrupt correlation changes at block borders.

\begin{figure}[!htb]
\caption{Testing on ten 100-time-point blocks, each with different correlations}
\includegraphics[width=1\textwidth]{../figures/SyntheticTesting/10block100t.png}
\label{fig:10block100t}
\small{\textbf{Figure Description:} In the first row, solid lines show the correlation between block correlations and TimeCorr results; dotted, sliding window results. In the second row, solid lines show mean squared error(MSE) between block correlations and TimeCorr results; dotted, sliding window results. In the first two rows, the different colors represent correlation and MSE between the true correlation in each block and the recovered results. In the third row, the solid line shows correlation between temporal ground truth and TimeCorr results; dotted, sliding window results. In the fourth row, the solid line shows MSE between temporal ground truth and TimeCorr results; dotted line, sliding window results. Finally, the columns represent TimeCorr implementations with a different Gaussian variances.}
\end{figure}

The graphs displayed in Figure \ref{fig:10block100t} show evaluation of the average results from running TimeCorr and the sliding window method on 100 different block correlation datasets, each containing 10 blocks of 100 time points. In the graphs on the first row, each color represents the correlation between the true correlation structure of a time block and the recovered results from TimeCorr and the sliding window approach, which are represented by solid and dotted lines, respectively. The graphs show that both the TimeCorr and the sliding window approach are able to easily differentiate the true block correlation structure from the correlations of other blocks. In addition, we can see that results from the TimeCorr approach show different levels of locality and stability with different configurations of Gaussian variance. At low Gaussian variance, the TimeCorr results show high correlation with ground truths at time points near the center of each block where the correlation structure remains unchanged, indicating better imitations of the general structures of the true correlation matrices. In contrast, results from TimeCorr setups with high Gaussian variance demonstrates lower correlation at time points where the correlations structure is stable. We hypothesize this difference may be due to the differences in temporal resolution between TimeCorr setups of varying Gaussian variance. As TimeCorr setups with lower Gaussian variance (higher temporal resolution) place more emphasis on time points closer to the time point of interest and neglect time points further away, their temporal results are less easily influenced by later changes in correlation structures.

The graphs in the second row show the MSE between the true correlation in each block and the TimeCorr and sliding window results, represented by solid and dotted lines, respectively. TimeCorr and the sliding window approach seem to have very similar performances when the TimeCorr variance is low. However, as the Guassian variance for TimeCorr coefficients generation increases, the MSE of the TimeCorr results shows a general decreasing trend, indicating that low resolution (high variance) TimeCorr setups produce approximations with values closer to that of the ground truths.

The graphs in the third and fourth row show the correlation and MSE between the temporal ground truth (true correlation at each time point) and the TimeCorr and sliding window results, represented by solid and dotted lines, respectively. As the Gaussian variance of TimeCorr increases, we observe big improvements in the correlation and the MSE between its results and the temporal ground truths at block borders, where there is abrupt changes in the temporal correlation structure. At time points where the correlation structure is stable and unchanging (block centers), results from TimeCorr setups with low variance demonstrate high correlation and low MSE with the temporal ground truths, which is consistent with the patterns in the graphs in the first two rows. These patterns confirm our earlier hypothesis that high resolution (low variance) TimeCorr setups demonstrates better performance when the correlation structure is stable, but TimeCorr setups with low resolution (high variance) is able to recover changing correlation patterns with better accuracy. The performance difference between high Gaussian variance and low Gaussian variance TimeCorr setups also demonstrates that utilization of information from a larger neighborhood of time points is conducive to more effective recovery of dynamically changing correlation structures, while concentrating on a smaller neighborhood of time points gives better recovery of stable correlation patterns.

\begin{figure}[!htb]
\caption{Testing on two 250-time-point blocks, each with different correlations}
\includegraphics[width=1\textwidth]{../figures/SyntheticTesting/2block250t.png}
\label{fig:2block250t}
\small{\textbf{Figure Description:} In the first row, solid lines show the correlation between block correlations and TimeCorr results; dotted, sliding window results; dot-dashed, ground truth. In the second row, solid lines show mean squared error(MSE) between block correlations and TimeCorr results; dotted, sliding window results; dot-dashed, ground truth. In the first two rows, the different colors represent correlation and MSE between the true correlation in each block and the recovered results. In the third row, the solid line shows correlation between temporal ground truth and TimeCorr results; dotted, sliding window results; gray, random guess. In the fourth row, the solid line shows MSE between temporal ground truth and TimeCorr results; dotted line, sliding window results; gray, random guess. Finally, The columns represent TimeCorr implementations with a different Gaussian variances.}
\end{figure}

To further confirm our results, we reran the tests on 100 different datasets containing 2 time blocks with different correlations, each spanning 250 time points. Evaluation of the average results from the TimeCorr and the sliding window implementations are shown in Figure \ref{fig:2block250t}, with behaviors of the ground truth and random guesses displayed for comparison. From these graphs, we can confirm that TimeCorr with low Gaussian variance is able to maintain high correlation and low MSE with the temporal ground truth at time points where the correlation remains unchanged. In contrast, TimeCorr with low resolution (high Gaussian variance) is able to produce slightly higher correlation and lower MSE across most time points. However, low resolution (high variance) TimeCorr predicts and reacts to future changes in temporal correlation with greater sensitivity, which results in premature changes in its results before the changes in ground truth actually take place.

In conclusion, as low Gaussian variance (high temporal resolution) TimeCorr implementations can achieve accurate and stable recovery of correlation structures that remain constant over extended blocks of time points and only suffer marginal accuracy loss at abrupt correlation shifts, it is more suitable for fMRI datasets where the dynamic correlation structures of brain activities are similar to that of the block correlation synthetic datasets. In addition, when compared with the sliding window approach, high resolution TimeCorr implementations also boast slightly better recovery performance in general with the additional advantage of no information loss.

\subsubsection{Single Subject Synthetic Dataset with ramping correlations}

In these datasets, we adopt a correlation structure that linearly changes from one terminal correlation $C_1$ to another terminal correlation $C_2$. These datasets will be used to test TimeCorr's ability to accurately recover correlation structures that are dynamically changing over time. To generate a ramping correlation dataset:
\begin{enumerate}
\item Using random normal distribution, generate a dataset X of dimensions $T$ by $V$, where $T$ and $V$ represent the desired time length and voxel (brain region) count, respectively.
\item Generate two terminal correlation matrices $C_1$ and $C_2$, then generate temporal correlation structures at time point $t$ using the function
\begin{align*}
C_t = z2r(\frac{(T-t) \cdot r2z(C_1)}{T} + \frac{t\cdot r2z(C_2)}{T})
\end{align*}
where T is the total time length of the datase. $r2z$ and $z2r$ represent Fisher Z-transformation and inverse Fisher Z-transformation, respectively. These transformations were applied to ensure stable combination of correlation structures.
\item For activations $X_t$ at each time point in X, transform the data into correlated structure through the process:
\begin{align*}
W_t = Cholesky(C_t) \cdot X_t
\end{align*}
\item Piece everything together to form the new synthetic dataset W that possesses the designated dynamic correlation structure.
\end{enumerate}

To evaluate and compare the performances of TimeCorr and the sliding window method in recovering the ground truth in ramping correlation datasets, we used four metrics: (1) correlation with the two terminal ground truths, (2) MSE with the two terminal ground truths, (3) correlation with the temporal ground truth, and (4) MSE with the temporal ground truth. The first two metrics were chosen to demonstrate how well TimeCorr and the sliding window method can recover the pattern of linear transition between the two terminal ground truths. The last two metrics were chosen to demonstrate the overall performance of TimeCorr and the sliding window approach in recovering the temporal ground truth.

\begin{figure}[!htb]
\caption{Testing different TimeCorr setups on 1000-timepoint ramping datasets}
\includegraphics[width=1\textwidth]{../figures/SyntheticTesting/ramp1000t4var.png}
\label{fig:ramp1000t4var}
\small{\textbf{Figure Description:} In the first row, solid lines show the correlation between the two terminal correlations and TimeCorr results; dotted, sliding window results; dot-dashed, ground truth. In the second row, solid lines show mean squared error(MSE) between the two terminal correlations and TimeCorr results; dotted, sliding window results; dot-dashed, ground truth. In the first two rows, the blue lines represent the relationship between recovered result and the left terminal correlation; red line, right terminal correlation. In the third row, the solid line shows correlation between temporal ground truth and TimeCorr results; dotted, sliding window results; gray, random guess. In the fourth row, the solid line shows MSE between temporal ground truth and TimeCorr results; dotted line, sliding window results; gray, random guess. Lastly, the columns represent TimeCorr implementations with a different Gaussian variances.}
\end{figure}

First, 100 ramping datasets with 1000 time points were generated to compare how TimeCorr and the sliding window method perform on dynamic correlation structures that gradually linear transitions between two distinct correlation structures. Evaluation of the average testing results are displayed in Figure \ref{fig:ramp1000t4var}. At Gaussian variance equal to 100, TimeCorr shows very similar performance with the sliding window method at a window length of 51. Both methods were able to accurately recover the linear transition patterns between the two terminal ground truth (correlation and MSE improves with respect to one terminal correlation but decreases with respect to the other terminal correlation, forming an obvious cross shape). However, as the Gaussian variance of TimeCorr increased, we observe overall improvements in both the correlation and MSE between the TimeCorr results and the temporal ground truths. This phenomenon is consistent with the patterns we observed in our block correlation synthetic dataset test results, where TimeCorr setups with high Gaussian variance is able to recover dynamically changing correlation structures with much higher accuracy than low variance TimeCorr implementations.

\begin{figure}[!htb]
\caption{Testing different TimeCorr setups on 500-time-point ramping datasets}
\includegraphics[width=1\textwidth]{../figures/SyntheticTesting/ramp500t4var.png}
\label{fig:ramp500t4var}
\small{\textbf{Figure Description:} In the first row, solid lines show the correlation between the two terminal correlations and TimeCorr results; dotted, sliding window results; dot-dashed, ground truth. In the second row, solid lines show mean squared error(MSE) between the two terminal correlations and TimeCorr results; dotted, sliding window results; dot-dashed, ground truth. In the first two rows, the blue lines represent the relationship between recovered result and the left terminal correlation; red line, right terminal correlation. In the third row, the solid line shows correlation between temporal ground truth and TimeCorr results; dotted, sliding window results; gray, random guess. In the fourth row, the solid line shows MSE between temporal ground truth and TimeCorr results; dotted line, sliding window results; gray, random guess. Finally, The columns represent TimeCorr implementations with a different Gaussian variances.}
\end{figure}

The performance advantage of low resolution (high variance) TimeCorr setups on recovering dynamically changing correlation structures is further verified by the results from testing on 100 ramping datasets of 500 time points. As the temporal correlation linearly transitions between the two terminal correlations within only 500 time points, the rate of change at each time point is much greater than that of the 1000 time point dataset. Evaluation of the average testing results are displayed in Figure \ref{fig:ramp500t4var}. In contrast to the results from the 1000 time point dataset, the performance difference between different resolution TimeCorr implementations is more evident in the results from the 500 time point datasets. When the Gaussian variance of TimeCorr is increased from 100 time points to 500 time points and beyond, we observe greater improvements in TimeCorr's performance---using both the correlation and the MSE evaluation metric---than in the results from the 1000 time point ramping dataset. This pattern confirms the advantage of incorporating information from a wider neighborhood of time points in the recovery of dynamically changing correlations structures. However, when we increased TimeCorr's Gaussian variance beyond the time length of the dataset (500 time points), we saw marginal improvements in TimeCorr's recovery performances. We hypothesize that because all the time points in the time series are already effectively utilized by TimeCorr when the Gaussian variance is equal to the time length of the dataset, increasing the Guassian variance further will have little to no effect on TimeCorr's result.

\begin{figure}[!htb]
\caption{Testing different TimeCorr setups on 100-time-point ramping datasets}
\includegraphics[width=1\textwidth]{../figures/SyntheticTesting/ramp100t4var.png}
\label{fig:ramp100t4var}
\small{\textbf{Figure Description:} In the first row, solid lines show the correlation between the two terminal correlations and TimeCorr results; dotted, sliding window results; dot-dashed, ground truth. In the second row, solid lines show mean squared error(MSE) between the two terminal correlations and TimeCorr results; dotted, sliding window results; dot-dashed, ground truth. In the first two rows, the blue lines represent the relationship between recovered result and the left terminal correlation; red line, right terminal correlation. In the third row, the solid line shows correlation between temporal ground truth and TimeCorr results; dotted, sliding window results; gray, random guess. In the fourth row, the solid line shows MSE between temporal ground truth and TimeCorr results; dotted line, sliding window results; gray, random guess. Finally, the columns represent TimeCorr implementations with a different Gaussian variances.}
\end{figure}

To further explore the characteristics of different resolution TimeCorr setups in recovering highly dynamic correlation structures, we conducted analysis on 100 ramping datasets of only 100 time points, where the linear transitions between the two distinct terminal correlation structures are further accelerated from the 500 time point datasets. Evaluation of the average testing results are displayed in Figure \ref{fig:ramp100t4var}, and confirm our hypothesis from the previous tests: the performance of TimeCorr bottlenecks when its Gaussian variance approaches the time length of the dataset. As the Gaussian of variance of TimeCorr exceeds the total time length of 100 time points, we start to see a deterioration in the cross pattern in the first row. In addition, the correlation and MSE between TimeCorr results and the temporal ground truths (displayed in row 3 and 4) sees little to no improvement after the Gaussian variance of TimeCorr exceeds 500 time points. We also noticed TimeCorr outperforms the sliding window approach in all four of its setups, indicating that the ability of TimeCorr to incorporate information from more time points makes it superior in recovering dynamically changing correlation structures than the sliding window approach.

Finally, summarizing the patterns observed from the test results across the 1000 timepoint, 500 time point and 100 time point datasets, we conclude that high Gaussian variance (low resolution) TimeCorr setups are advantageous in recovering continuously changing correlation structures. In addition, low resolution TimeCorr implementations show optimal performance when its Gaussian variance is equal to the total time length of the dataset. Thus, if the correlation dynamics of an fMRI dataset shows continuous change instead stable blocks, it is most advantageous to use a TimeCorr setup with a Gaussian variance equal the lesser between the total time length and 1000 time points for recovery of its dynamic correlation structure.

Lastly, to gain a more comprehensive understanding of how TimeCorr performs relative to sliding window implementations of different window lengths, we conducted an analysis on 100 ramping datasets, each containing 1000 time points. Evaluation of the average testing results are displayed in Figure \ref{fig:ramp1000t4slide}. The TimeCorr setup used for comparison has a Gaussian variance equal to the total time length of the dataset (1000 time points), a setup previous proven to produce the best recovery results on ramping datasets. Four sliding window setups were implemented each with window lengths equal to 11, 25, 51 and 101 time points. From the graphs, we can see that the performance of sliding window generally increases as the window length increases, but the rate of improvement decreases dramatically after the window length reaches 51 time points (marginal improvement when an additional 50 time points is added to the window length). In addition, the performance of the sliding window approach at a window length of 11 time points is underwhelming in comparison with the TimeCorr approach. As the window length increases to 101 time points, the correlation and MSE between the sliding window result and temporal ground truths gradually nears the performance of the standard TimeCorr implementation. However, a window length of 101 time points is equivalent to around $10\%$ of the time length of most fMRI datasets. Losing $10\%$ of the information for calculation of functional connectivity at each level is impractical for explorations of high level brain dynamics. For the above reasons, we chose TimeCorr as the more preferable method of functional connectivity calculation for our High-Order Brain Dynamics Model.

\begin{figure}[H]
\caption{Testing different sliding window setups on 1000-time-point ramping datasets}
\includegraphics[width=1\textwidth]{../figures/SyntheticTesting/ramp1000t4slide.png}
\label{fig:ramp1000t4slide}
\small{\textbf{Figure Description:} In the first row, solid lines show the correlation between the two terminal correlations and TimeCorr results; dotted, sliding window results; dot-dashed, ground truth. In the second row, solid lines show mean squared error(MSE) between the two terminal correlations and TimeCorr results; dotted, sliding window results; dot-dashed, ground truth. In the first two rows, the blue lines represent the relationship between recovered result and the left terminal correlation; red line, right terminal correlation. In the third row, the solid line shows correlation between temporal ground truth and TimeCorr results; dotted, sliding window results; gray, random guess. In the fourth row, the solid line shows MSE between temporal ground truth and TimeCorr results; dotted line, sliding window results; gray, random guess. Finally, the columns represent different sliding window implementations with varying window lengths.}
\end{figure}

\subsection{Testing Inter-Subject Functional Connectivity using TimeCorr}
Confident with TimeCorr's ability to recover the dynamic correlation structures of single subject datasets, we proceeded to test TimeCorr's performance in recovering inter-subject functional connectivity (ISFC) in multi-subject synthetic datasets. In practical scenarios, fMRI datasets always contain a significant amount of noise from non stimulus-driven neural activities. Therefore, to simulate realistic human brain activities, we added different levels of random noise to the synthetic datasets to gauge the robustness of TimeCorr ISFC in recovering stimulus-related functional connectivity. The noise levels we experimented with were on the order of magnitudes of $10\%$ (0.1), $100\%$(1), $1000\%$(10) and $10000\%$(100) of the average activation magnitude. Evaluation of results from running TimeCorr with Gaussian variance of 1000 time points and a sliding window setup with a window length of 51 time points on 100 ramping datasets, each containing 1000 time points, are demonstrated in Figure \ref{fig:t300slide25var1000}.

\begin{figure}[!htb]
\caption{Testing ISFC on 1000-time-point ramping datasets with different noise levels}
\includegraphics[width=1\textwidth]{../figures/ISFC/t300slide25var1000.png}
\label{fig:t300slide25var1000}
\small{\textbf{Figure Description:} In the first row, solid lines show the correlation between the two terminal correlations and TimeCorr results; dotted, sliding window results; dot-dashed, ground truth. In the second row, solid lines show mean squared error(MSE) between the two terminal correlations and TimeCorr results; dotted, sliding window results; dot-dashed, ground truth. In the first two rows, the blue lines represent the relationship between recovered result and the left terminal correlation; red line, right terminal correlation. In the third row, the solid line shows correlation between temporal ground truth and TimeCorr results; dotted, sliding window results; gray, random guess. In the fourth row, the solid line shows MSE between temporal ground truth and TimeCorr results; dotted line, sliding window results; gray, random guess. Finally, the columns represent recovery results from datasets with varying noise levels.}
\end{figure}

The graphs show that TimeCorr is able to retrieve the stimulus-driven dynamic correlation structure with relatively high accuracy at low (0.1) to medium (1) noise levels, but fails to recover any meaningful information when the noise level is too high (10 and 100). These results highlight the difficulty of recovering stimulus-driven correlation structures when the subjects display weak cognitive response to the stimulus. However, when the stimuli is able to evoke reasonably strong cognitive response from the subject, TimeCorr ISFC can effectively recover the stimuli-driven dynamic correlation structures with relatively high accuracy.

In addition, due to the limited number of total time points in the dataset, we chose to use a sliding window setup with a window length of 51 time points. When comparing the correlation and MSE between their results and the temporal ground truths, the TimeCorr approach distinctly outperforms the sliding window method in every situation, even when the noise level is high. Furthermore, as the amount of noise increases, the performance of the sliding window approach deteriorates more noticeably than that of the TimeCorr approach. These contrasts further consolidate the advantageous ability of TimeCorr to produce more stable and accurate recovery of dynamic correlation structures by incorporating a wider range of time points, making it the superior choice for functional connectivity calculations.

\clearpage
\newpage
\subsection{HOBD Intra-Level Decoding Analysis}
The analyses on synthetic datasets demonstrated the advantages of using TimeCorr for the recovery of single-subject functional connectivities and ISFC from noisy synthetic datasets. These advantages enable the HOBD model to explore brain dynamics through the lens of high-order functional connectivities. For the next step, we conducted the HOBD Intra-Level Decoding Analysis (ILDA), which evaluates how much stimulus-driven brain dynamics information is contained within each level of high-order ISFC obtained using the HOBD recovery methods (TimeCorr and TimeCorr Level Up). Unlike previous tests with synthetic datasets, the ground truths for each order of functional connectivity in actual fMRI datasets are unknown. Therefore, we used the decoding accuracy as a noisy proxy to roughly estimate the quality of functional connectivity recoveries and the quantity of stimulus-driven information within each order of functional connectivity. Firstly, for each subject, we used the TimeCorr Level Up function on their PCA reduced activations to calculate 9 levels of high-order functional connectivity as new activations of the corresponding order (e.g. for each subject, $9^{th}$ order functional connectivity is redefined as $9^{th}$ order activations). Next, we conducted decoding analysis on the ISFC of the subject activations at each order, from PCA reduced raw activations ($0^{th}$ order ISFC) and its ISFC ($1^{st}$ order ISFC) to the ISFC of the 9th order functional connectivities/activations ($10^{th}$ order ISFC). Three TimeCorr setups were implemented---each with Gaussian variances of $\text{min}(1000,\text{time length})$ time points, 10 time points, or 1 time point---to comprehensively understand the decoding capability of each level under varying resolutions.

Three different datasets were used for this analysis: Sherlock, Pieman, and Forrest. Detailed descriptions for each dataset are included in each subsection. For each dataset, we conducted 100 repetitions of decoding analysis with different random group divisions for each TimeCorr setup. All of the following results are the average decoding accuracy of 100 repetitions.

\subsubsection{Description for the Pieman Dataset}
This dataset was collected by Uri Hasson's lab in 2016 \citep{hasson2016}. The stimuli for this experiment were derived from a 7 minute recording of the ``Pie Man" story by Jim O'Grady. The experimental subjects were all native English speakers with normal hearing. Four testing conditions with random subject assignments were implemented: 36 subjects listened to the story from beginning to end, and were labelled as the ``intact" group; 18 subjects listened to a version of the story where the paragraphs were scrambled randomly, and were labelled as the ``paragraph" group; 36 subjects listened to a version of the story where the words were scrambled randomly, and were labelled as the ``word" group; lastly, 36 subjects did not listen to anything and stayed in resting state throughout the experiment, and were labelled as the ``resting" group. 12 seconds of neutral music and 3 seconds of silence preceded and 15 seconds of silence followed each playback in all conditions. The brain activities of all subjects for the duration of the experiment were recorded into fMRI datasets, with the data from music and silence periods discarded from all analyses.

\subsubsection{Description for the Forrest Dataset}
This dataset was collected by Michael Hanke and coleagues in 2014\citep{Hanke2014}. The stimuli for this experiment is a two-hour presentation of an audio-only version of the Hollywood feature film ``Forrest Gump" made specifically for a visually impaired audience. The movie was divided into eight 15-minute sessions with breaks in-between, and presented to 20 subjects with normal hearing. During each session, brain activities centered on the auditory cortices in both brain hemisphere and the frontal and posterior portions of the brain were recorded into high resolution 7-Tesla fMRI datasets.

\subsubsection{Description for the Sherlock Dataset}
This dataset was collected by Uri Hasson's Lab in 2017 \citep{Chen2017}. Sixteen participants (17 were described in the original paper, but one was removed from this dataset due to a small amount of missing data at the end of the movie scan) were presented with a 50-minute segment of the audio-visual movie Sherlock (BBC) while undergoing fMRI scan. Following the movie, participants were asked to describe, without any external guidance, what they recalled of its content in as much detail as they could while undergoing brain imaging. Participants were allowed to speak on any aspect of the movie for as long as they wished, while their speech was recorded with an fMRI-compatible microphone.

\subsubsection{Results}
\begin{enumerate}
\item The average results for each TimeCorr setup implemented on the Pieman dataset are displayed in Figures \ref{fig:piemanDC1000}, \ref{fig:piemanDC10}, and \ref{fig:piemanDC1}.
\item The average results for each TimeCorr setup implemented on the Forrest dataset are displayed in Figure \ref{fig:forrestDC1000}, \ref{fig:forrestDC10}, and \ref{fig:forrestDC1}.
\item The average results for each TimeCorr setup implemented on the Sherlock dataset are displayed in Figure \ref{fig:sherlockDC1000}, \ref{fig:sherlockDC10}, and \ref{fig:sherlockDC1}.
\end{enumerate}

\begin{figure}[!htb]
\caption{ILDA on TimeCorr with variance equal to total time length (Pieman)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/1000V/pieman.png}
\label{fig:piemanDC1000}
\caption{ILDA on TimeCorr with variance of 10 time points (Pieman)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/10V/pieman.png}
\label{fig:piemanDC10}
\caption{ILDA on TimeCorr with variance of 1 time point (Pieman)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/1V/pieman.png}
\label{fig:piemanDC1}
\end{figure}

Firstly, when comparing our results in Figures \ref{fig:piemanDC1000}, \ref{fig:piemanDC10}, and \ref{fig:piemanDC1} with that previous efforts, we can see that the decoding accuracy we achieved on the first order ISFC for the Pieman dataset using TimeCorr-based methods is much higher across all the TimeCorr setups \citep{jeremy2017}. As higher decoding accuracy is a strong indication of the richness of stimulus-driven activities within a dataset, our improved results confirm that TimeCorr-based methods are superior in the recovery of stimulus-driven correlation structure from fMRI datasets. Furthermore, the decoding accuracy we achieved on PCA reduced fMRI data is significantly higher than the decoding accuracy obtained from HTFA reduced datasets, indicating that PCA is more effective in preserving stimulus-driven brain dynamics than HTFA.

Secondly, we noticed that the decoding accuracy of the first order ISFC exceeds the decoding accuracy of PCA reduced brain activations in some situations, suggesting that there might be more stimulus-related brain dynamics information contained within functional connectivities than in raw activations. However, we also noticed that the decoding accuracies of higher-order ISFCs fall off almost exponentially after the first order ISFC, a behavior that's also observed in the results from the Sherlock and the Forrest datasets. Although there is a possibility that less brain dynamics information is contained at these levels, we speculate that the main factor causing the decrease in decoding accuracy is information loss, which can be caused by PCA dimensionality reduction and/or TimeCorr Gaussian averaging.

To test the effect of TimeCorr Gaussian averaging on information loss at each "level up", we implemented three TimeCorr setups with different Gaussian variances. The results from the Pieman dataset show that the decoding accuracy of lower orders of ISFC increases drastically when the TimeCorr Gaussian variance is decreased from the dataset time length to 10 time points, but undergoes a decrease when the Gaussian variance is decreased further to 1 time point. Additionally, we observe that, as the TimeCorr Gaussian variance decreases from the dataset time length to 1 time point, the decoding accuracy of higher order ISFCs of the Pieman dataset seem to show a marginal increase, indicating that Gaussian averaging inside TimeCorr may be one of the causes for information loss at high levels. However, the increase in decoding accuracy for low variance TimeCorr implementations is not replicated by the results from the Forrest nor the Sherlock dataset, which suggests that Gaussian averaging may not be the biggest factor causing the information loss.

Lastly, when comparing between results under different stimulus conditions (intact, paragraph, word, resting) within the Pieman dataset, we can see that as the complexity and integrity of the stimulus decreases, the decoding accuracy decreases for all orders of functional connectivity. This makes sense as the human brain should intuitively produce more potent cognitive response to more coherent and meaningful stimuli (intact), weaker response to scrambled and irrational stimuli (paragraph, word), and little to no response to no stimuli (resting). One anomaly exists in the results from the TimeCorr implementation with a Gaussian variance of 1 time point, where the decoding accuracy of the first order ISFC in the paragraph condition exceeded that of the intact condition. However, this behavior becomes understandable after taking into consideration the results from the TimeCorr implementation with a Gaussian variance of 10 time points: while the decoding accuracy for the first order ISFC of the paragraph condition only showed marginal decrease, the decoding accuracy of the first order ISFC of the intact condition decreased drastically when the TimeCorr Gaussian variance changed from 10 time points to 1 time point. When the temporal resolution of TimeCorr is too high (Gaussian variance of 1 time point), TimeCorr will tunnel-vision on a very narrow neighborhood of time points for its functional connectivity calculations, and thus fail to incorporate enough information to recover any meaningful dynamic correlation patterns. As the functional connectivity under the intact condition possesses more structure and coherency, it may be suffer greater deterioration than the functional connectivity from the paragraph condition when subjected to over-high temporal resolution, thus explaining the anomaly.

\begin{figure}[!htb]
\caption{ILDA on TimeCorr with variance equal to total time length (Forrest)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/1000V/forrest.png}
\label{fig:forrestDC1000}
\caption{ILDA on TimeCorr with variance of 10 time points (Forrest)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/10V/forrest.png}
\label{fig:forrestDC10}
\caption{ILDA on TimeCorr with variance of 1 time point (Forrest)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/1V/forrest.png}
\label{fig:forrestDC1}
\end{figure}

The results from the Forrest dataset, shown in Figures \ref{fig:forrestDC1000}, \ref{fig:forrestDC10}, and \ref{fig:forrestDC1}, show very similar patterns as the results from the Pieman dataset. We observe that the decoding accuracy of first order ISFC increases greatly when the Gaussian variance is decreased from the dataset time length to 10, but decreases when the Gaussian variance is decreased further to 1. Also, when the Gaussian variance is equal to 10 time points and 1 time point, we observe that the decoding accuracy of the first order ISFC exceeds that of the PCA reduced brain activations, which confirms our previous hypothesis that high order brain connectivities may contain an unexpected amount of information on brain dynamics. Finally, the decrease in decoding accuracy for high order ISFCs is consistent with the results from the Pieman dataset, suggesting that there is a high probability of information loss at each ``level up". However, unlike the results from the Pieman dataset, we do not see an increase in decoding accuracy when the TimeCorr variance is decreased from the total time length to 1 time point, which verifies our previous speculation that Gaussian averaging in TimeCorr is not the main cause of information loss.

\begin{figure}[!htb]
\caption{ILDA on TimeCorr with variance equal to total time length (Sherlock)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/1000V/sherlock.png}
\label{fig:sherlockDC1000}
\caption{ILDA on TimeCorr with variance of 10 time points (Sherlock)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/10V/sherlock.png}
\label{fig:sherlockDC10}
\caption{ILDA on TimeCorr with variance of 1 time point (Sherlock)}
\centering
\includegraphics[width=0.5\textwidth]{../figures/Intra-Level-Decoding-Analysis/1V/sherlock.png}
\label{fig:sherlockDC1}
\end{figure}

The results from the Sherlock dataset, shown in Figures \ref{fig:sherlockDC1000}, \ref{fig:sherlockDC10}, and \ref{fig:sherlockDC1}, demonstrate slightly different patterns from the results from the Forrest dataset. We observe that the decoding accuracy of first order ISFC not only increases greatly when the Gaussian variance is decreased from the dataset time length to 10 time points, but also increases further when the Gaussian variance is decreased to 1 time point. However, like the results from the Pieman and the the Forrest datasets, we observe that the decoding accuracy of the first order ISFC exceeds that of the PCA reduced brain activations when the Gaussian variance is equal to 10 time points and 1 time point. We also observe an exponential decrease in decoding accuracy for high-order functional connectivities, which barely changes when the Gaussian variance of TimeCorr is decreased from dataset time length to 1 time point.

To summarize, one important phenomenon that is observed in the results from all three datasets is the decoding accuracies from the TimeCorr implementations with a Gaussian variance of 10 time points are generally higher that that of the TimeCorr implementations with Gaussian variance equal to the dataset time length. As higher decoding accuracy is indicative of functional connectivities of a greater proportion of time points being correctly recovered, the conclusions we drew from synthetic dataset testing tell that us that the dynamic correlation patterns in the Pieman, Forrest and Sherlock datasets may more closely resemble that of the block correlations synthetic datasets. However, as discussed previously, when the TimeCorr's temporal resolution is too high (1 time point), it may fail to include enough global information to recovery meaningful dynmaic correlation structures (Pieman and Forrest).

Another important observation that is consistent across results from all three datasets is that the higher order ISFCs actually produced lower decoding accuracy. This result was unexpected as brain dynamics across subjects should intuitively become more similar at higher orders. One possible explanation for this is, although the high-order ISFC still contain information about stimulus-driven brain activities, there seems to have been a small amount of information loss in each level up process, leading to slightly lower classification accuracy. However, although higher order ISFCs display weaker decoding capabilities than ISFCs at lower level, our hypothesis is that each order of ISFC contains distinct information about brain dynamics. If this is the case, then a mixture of all orders of ISFC will potentially have stronger decoding performance than that of ISFC at any single level.

We also suspected that TimeCorr temporal resolution had some influence on the results, which is why our analysis of each dataset consists of a variety of different TimeCorr setups. We hypothesize that if the Gaussian averaging process in TimeCorr caused information loss at each ``level up", the loss would intuitively grow exponentially as we reach higher orders of functional connectivity. Therefore, TimeCorr with Gaussian variances of dataset time length, 10 time points and 1 time point were chosen to explore if variation in TimeCorr temporal resolution would affect the distribution of decoding accuracy across different levels. However, looking at the results across all three datasets, we do not notice a significant decrease in the rate of decoding accuracy deterioration when the TimeCorr resolution shifts from a variance of dataset time length to a variance of 1 time point. On the contrary, there does not seem to be a linear relationship between the decoding accuracy at higher levels and the temporal resolution of TimeCorr: e.g. the decoding accuracy of first order ISFC in the Pieman dataset increased when TimeCorr variance decreased from dataset time length to 10 time points, but decreased when the TimeCorr variance decreased further to 1 time point (a phenomenon also observed in the Forrest results). These observations indicate that the Guassian averaging process in TimeCorr is not the main culprit of information loss.

\subsection{HOBD Multi-Level Mixture Analysis}
Although the HOBD ILDA revealed that information of high-order brain dynamics may not have been properly represented by the HOBD model due to information loss, it presented the possibility that the functional connectivities at each level may represent non-overlapping sources of stimulus-relevant cognitive information that could potentially complement each other. Therefore, we designed the Multi-Level Mixture Analysis (MLMA). This analysis constructs a mixture model incorporating information from multiple orders of ISFC, and compares its decoding capabilities with that of ISFC at each individual level.

Given a fMRI dataset for $S$ subjects, each possessing $T$ time points and $V$ voxels, to construct the Multi-Level Mixture Model:
\begin{enumerate}
\item Choose the desired number of levels of functional connectivities $L$ and the appropriate Gaussian variance for TimeCorr.
\item Use the TimeCorr Level Up method to obtain all level functional connectivites for each subject.
\item Randomly divide the subjects in to four equal groups $A_1$, $A_2$, $B_1$ and $B_2$.
\item For each group, use the Multi-Subject TimeCorr ISFC method to compute the inter-subject functional connectivity for each level
\item Compute correlation matrices for each level for A and B (i.e. for group A, correlate $A_1$'s features at each time point with $A_2$'s features at each time point; similarly for group B). This yields one correlation matrix for group A and another for group B, for each level (including level 0). So if $L=10$, this process would yield 11 correlation matrices for A and another 11 for B.
\item Apply Fisher Z-transform (r2z) on all of the correlation matrices. The transformed correlation matrices are labelled $z_{A}^0, z_{A}^1, z_{A}^2, ..., z_{B}^0, z_{B}^1, z_{B}^2, ..., z_{B}^{10}$, where $z_{A}^0$ represent the Fisher Z-transformed correlation matrix for level 0 of group A.
\item Use constrained optimization by linear approximation (COBYLA) on the correlation matrices of group A to compute the optimal weight matrix w to be used for decoding, where $w$ will be applied to the Fisher Z-transformed matrices in the manner of the following equation:
\begin{align*}
\text{decoding\_matrix\_A} = z2r(w[0]*z_A^0 + w[1]*z_A^1 + w[2]*z_A^2 + ... + w[10]*z_A^{10})
\end{align*}
where $z2r$ is the Inverse Fisher Z-transformation to convert the weighted sum into the its proper correlation form. Intuitively, this step seeks to find the w that maximizes the decoding accuracy using decoding\_matrix\_A.
\item Use the same z2r(weighted z-transformed sum) formula and the optimal weights from the previous step to compute decoding accuracy for group B
\item Output w and group B's decoding accuracy
\end{enumerate}

Three TimeCorr setups of varying temporal resolutions were implemented---each with Gaussian variances equal to $\text{min}(\text{dataset time length},1000)$ time points, 10 time points, or 1 time point---to comprehensively understand the decoding capability of the mixture model and each individual level under varying resolutions. In addition, similar to the single level decoding analysis conducted previously, three different datasets were used for this analysis: Sherlock, Pieman, and Forrest. For each TimeCorr setup for each dataset, we conducted 100 repetitions of decoding analysis with different random group divisions. All of the following results are the average of 100 repetitions.

Results:
\begin{enumerate}
\item The average results for each TimeCorr setup implemented on the Pieman dataset are displayed in Figure \ref{fig:piemanMM1000}, \ref{fig:piemanMM10}, and \ref{fig:piemanMM1}.
\item The average results for each TimeCorr setup implemented on the Forrest dataset are displayed in Figure \ref{fig:forrestMM1000}, Figure \ref{fig:forrestMM10} and Figure \ref{fig:forrestMM1}.
\item The average results for each TimeCorr setup implemented on the Sherlock dataset are displayed in Figure \ref{fig:sherlockMM1000}, Figure \ref{fig:sherlockMM10} and Figure \ref{fig:sherlockMM1}.
\end{enumerate}

\begin{figure}[!htb]
\caption{MLMA on TimeCorr with variance equal to total time length (Pieman)}
\centering
\includegraphics[width=1\textwidth]{../figures/Multi-Level-Mixture-Analysis/1000V/pieman-weights.png}
\includegraphics[width=1\textwidth]{../figures/Multi-Level-Mixture-Analysis/1000V/pieman-accuracy.png}
\label{fig:piemanMM1000}
\end{figure}

\begin{figure}[!htb]
\caption{MLMA on TimeCorr with variance of 10 time points (Pieman)}
\centering
\includegraphics[width=1\textwidth]{../figures/Multi-Level-Mixture-Analysis/10V/pieman-weights.png}
\includegraphics[width=1\textwidth]{../figures/Multi-Level-Mixture-Analysis/10V/pieman-accuracy.png}
\label{fig:piemanMM10}
\end{figure}

\begin{figure}[!htb]
\caption{MLMA on TimeCorr with variance of 1 time point (Pieman)}
\centering
\includegraphics[width=1\textwidth]{../figures/Multi-Level-Mixture-Analysis/1V/pieman-weights.png}
\includegraphics[width=1\textwidth]{../figures/Multi-Level-Mixture-Analysis/1V/pieman-accuracy.png}
\label{fig:piemanMM1}
\end{figure}

\begin{figure}[!htb]
\caption{MLMA on TimeCorr with variance equal to total time length (Forrest)}
\centering
\includegraphics[width=0.8\textwidth]{../figures/Multi-Level-Mixture-Analysis/1000V/forrest.png}
\label{fig:forrestMM1000}
\caption{MLMA on TimeCorr with variance of 10 time points (Forrest)}
\centering
\includegraphics[width=.8\textwidth]{../figures/Multi-Level-Mixture-Analysis/10V/forrest.png}
\label{fig:forrestMM10}
\caption{MLMA on TimeCorr with variance of 1 time point (Forrest)}
\centering
\includegraphics[width=.8\textwidth]{../figures/Multi-Level-Mixture-Analysis/1V/forrest.png}
\label{fig:forrestMM1}
\end{figure}


\begin{figure}[!htb]
\caption{MLMA on TimeCorr with variance equal to total time length (Sherlock)}
\centering
\includegraphics[width=.8\textwidth]{../figures/Multi-Level-Mixture-Analysis/1000V/sherlock.png}
\label{fig:sherlockMM1000}
\caption{MLMA on TimeCorr with variance of 10 time points (Sherlock)}
\centering
\includegraphics[width=.8\textwidth]{../figures/Multi-Level-Mixture-Analysis/10V/sherlock.png}
\label{fig:sherlockMM10}
\caption{MLMA on TimeCorr with variance of 1 time point (Sherlock)}
\centering
\includegraphics[width=.8\textwidth]{../figures/Multi-Level-Mixture-Analysis/1V/sherlock.png}
\label{fig:sherlockMM1}
\end{figure}

With each implementation of the Multi-Level Mixture Model, the subjects are divided into two equal groups, one for training and one for testing. As a result, we observe that training and testing using only half of the subjects caused the decoding accuracy to be a lot lower than when using the entire dataset. From a mathematical perspective, averaging over more subjects brings more stability to the dataset by smoothing out random noises in the dataset and adding emphasis to activities common to all subjects, which are typically stimulus-driven. Therefore, when less subjects are used in the calculation of inter-subject functional connectivity (ISFC), the de-noising effect of averaging is weakened and less emphasis is put on activities that are stimulus-driven, thus causing lower decoding accuracy.

One of the most important observations from the results of Multi-Level Mixture Analysis across all three datasets is that the Mixture Model produces much higher decoding accuracy than any single level of ISFC. When taking into consideration our hypothesis from the decoding analysis in the previous section, this is a very direct and compelling proof that different orders of ISFC, although not recovered with perfect fidelity, still contains important non-overlapping information about brain dynamics and, although individual levels did not produce high classification accuracy, when used in unison can greatly enrich data complexity and increase decoding capabilities.

The importance of each level is further confirmed by optimal weight distribution across all three datasets. While the first order ISFC always has the highest weight, the weight distribution between PCA reduced fMRI activations and higher order ISFCs is almost uniform, indicating their equal contribution to the enrichment of stimulus-related information and the improvement in the decoding capability of the dataset. Additionally, we noticed that the optimal weight of the first order ISFC was much higher than the rest of the levels, suggesting that a greater amount of stimulus-driven brain dynamics information is contained in this level. The significance of the first order ISFC in the mixture model is also consistent with high resolution results in HOBD ILDA, where first level ISFC achieved the highest decoding accuracy among all levels.

Like in HOBD ILDA, we were curious if changing TimeCorr Gaussian variance (changing temporal resolution loss across levels) would influencing the results, therefore we repeated the Multi-Level Mixture Analysis on each dataset with multiple TimeCorr setups. However, the optimal weight distribution qualitatively remained the same in every implementation: the first level ISFC always possesses the highest weight, while other levels always possesses similar weight. This further confirms that, although PCA reduced fMRI activations may sometimes produce higher decoding accuracy at low resolutions, first order ISFC actually contains the most information on brain dynamics that are stimulus-related.


\clearpage
\newpage
\section{Conclusion}
We proposed the High-Order Brain Dynamics Model, a new way to understand brain dynamics by incorporating information on high-order inter-subject functional connectivity into our analysis. In the process, we proposed the TimeCorr method, a lossless alternative to the sliding window approach for functional connectivity calculation, and the TimeCorr Level Up method, a new means of calculating high-order functional connectivity. Through testing on synthetic data, we demonstrated the superiority of the TimeCorr approach over the sliding window approach in recovering temporal functional connectivity, as well as its ability to differentiate between datasets with different correlation dynamics (block vs ramping). With TimeCorr and Level Up, we had the means to peer into the secrets of the brain through the lens of the High-Order Brain Dynamics Model. First, we calculated 10 orders of functional connectivity from the fMRI data of 3 past studies (Pieman, Forrest and Sherlock). Then, we carried out decoding analysis individually for each order of functional connectivity to understand how well stimulus-driven brain dynamics information is represented at each level. Afterwards, we conducted the Multi-Level Mixture Analysis to understand how information at each order of functional connectivity complements each other. Finally, as the decoding accuracy for all three datasets across all levels were generally better for TimeCorr implementations with Gaussian variance of 10 time points instead of $\text{min}(1000,\text{dataset time length})$ time points, we extrapolated from our synthetic dataset testing results that the brain activities in the Pieman, Sherlock and Forrest datasets may consist of multiple different length time blocks, each possessing stable correlation structures within its scope. Christopher Baldassano and colleagues confirmed this extrapolation for the Pieman and the Sherlock datasets in one of their recent papers \citep{Baldassano2016}.

Through the lens of the High-Order Brain Dynamics Model, we were able analyze the patterns in higher orders of functional connectivity and achieve a better understanding of stimulus-driven brain dynamics in existing datasets (Pieman, Forrest and Sherlock). In addition, through the construction of the Multi-Level Mixture Model, we were able to find effective ways to optimally combine stimulus-relevant information from multiple orders of functional connectivities into one functional connectivity matrix. This process enriches the amount of stimulus-relevant information in existing dataset without creating redundancies, which greatly increases the brain decoding capabilities of these dataset and has practical applications in many areas.

Firstly, this technology can potentially be used for mental evaluations in medical diagnosis and criminal investigations. After collecting the cognitive responses to certain stimuli of normal subjects, we can use the HOBD model to create a more comprehensive representation of their brain dynamics using information from high-order functional connectivity, as well as calculate the average range of decoding accuracies of the normal population. Then, we can use the HOBD model to create a similar representation of the cognitive response of any new subjects to the same stimuli. By using the decoding analysis to cross validate the high-order functional connectivity representation of the subject's brain responses with the ISFC of normal subjects, we can compare the resulting decoding accuracy with the previously calculated range for normal subjects to detect any cognitive anomalies in the subject's responses. This will help us identify any potential psychological or mental disorders, or suspicious reactions to crime scene reenactments. Although similar technologies already exist, we believe by constructing a more complex but comprehensive representation of high-order brain activities, we are inherently using more criteria for classification and are thus able obtain higher classification accuracy.

Secondly, as the HOBD model is able to generate more stimulus-relevant information from brain activities through analysis of high-order functional connectivities, we are able to produce more feature points to represent the state of the brain at each time point. Having a more complex feature map and a more comprehensive representation of the underlying temporal activities of the brain will increase the ability of Machine Learning algorithms to learn the patterns and relationships between the stimulus and the corresponding temporal brain activities. The trained Machine Learning models can then be used to recreate stimuli (visual, audio, etc) from HOBD representations of new brain activities, which could potentially give us more knowledge on human perception patterns.

Although we were able to increase the amount of stimulus-driven brain dynamics information retrievable from fMRI datasets through the use of TimeCorr and TimeCorr Level Up, we believe the abrupt decrease in decoding accuracy following the second order ISFC does not accurately represent the amount of information contained in these levels and is indicative of information loss. As our tests proved that TimeCorr is not the only cause for the decrease in decoding accuracy, we hypothesize that the information loss may also have been partially caused by PCA dimensionality reduction in the TimeCorr Level Up processes.

To address this issue and to further improve the High-Order Brain Dynamics Model in the future, we intend to experiment with the following options:

Firstly, we now have more confidence that the brain activities of subjects in the Pieman, Forrest and Sherlock datasets on average mainly consists of time blocks of stable correlation structures. Therefore, to better calculate the temporal functional connectivity at each time point, using only activations of time points within the scope of the corresponding correlation time block could significantly improve the accuracy of our calculations. One difficulty to this approach is the detection of block borders where the abrupt correlation changes occur. Christopher Baldassano and colleagues have tried using a variant of the Hidden Markov Model to detect the transitions between time blocks and achieved good results \citep{Baldassano2016}. We believe implementing this approach into our HOBD model could very likely improve the quality of our functional connectivity recoveries.

Secondly, we plan to experiment with other dimensionality reduction algorithms, like Spacial ICA. Although PCA is able to efficiently combine brain voxels with similar activity patterns and effectively preserve the general correlation dynamics within a dataset, it loses considerable amounts of important information by neglecting spatial distinctions between voxels. The correlation between voxels with similar activity patterns may be disposable when the voxels are from the same brain region, but the correlation between similar voxels across different brain regions could potentially contain very important information on the stimulus-driven dynamics of the brain. Alternative methods like spacial ICA are not only able to effectively reduce dataset dimensionality like the PCA method, but also does so while taking into consideration the spacial distinction between voxels. Spacial ICA has seen applications in multiple projects and has achieved good results \citep{Dipasquale2015}\citep{Iraji2016}\citep{Xu2013}, so we believe it could potentially be an effective alternative to PCA in our HOBD model.

To conclude, recovering high-order functional connectivity patterns is a challenging task, but is also crucial for achieving a more in-depth and comprehensive understanding of stimulus-driven brain dynamics. We introduced the HOBD model as a new means to examine and incorporate useful information from high-order functional connectivities into our analysis. Using this model, we were able to discover that important information for decoding the brain is contained within these latent structures. We believe the applications and benefits of high-order brain connectivities are endless, and we are excited to explore new approaches to improve the HOBD model and expand the horizon of human brain decoding.

\newpage
\bibliographystyle{apalike}
\bibliography{HOBD}
\end{document}
